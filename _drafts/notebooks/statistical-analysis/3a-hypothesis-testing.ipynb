{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38032bit64a64ed7a47843b8be3706a54e9a0958",
   "display_name": "Python 3.8.0 32-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# HYPOTHESIS TESTING\n",
    "\n",
    "## Goal \n",
    "\n",
    "Understanding relationships between response and predictors is about testing two opposite hypotheses:\n",
    "+ null hypothesis $H_0$: there is no relationship between a predictor and a response.\n",
    "+ alternative hypothesis: there is a relationship.\n",
    "\n",
    "\n",
    "## Experimental Design\n",
    "\n",
    "Hypothesis testing is used to **make decisions about a population** using sample data. \n",
    "\n",
    "+ We start with a **null hypothesis $H_0$** that we we asssume to be true. For instance:\n",
    "    + the sample parameter is equal to a given value.\n",
    "    + samples with different characteristics are drawn from the same population.\n",
    "+ We run an **experiment** to test this hypothesis:\n",
    "    + **collect data** from a sample of predetermined size.\n",
    "    + perform the appropriate **statistical test**.\n",
    "+ Based on the experimental results, we can either **reject** or **fail to reject** this null hypothesis. \n",
    "+ If we reject it, we say that the data supports another, mutually exclusive, **alternate hypothesis**.\n",
    "\n",
    "\n",
    "## Test Distribution\n",
    "\n",
    "A statistical test is based on some assumption regarding the sampling distribution of the test statistics under the null hypothesis; The assumptions depend on the test. \n",
    "\n",
    "\n",
    "## P-value\n",
    "\n",
    "We measure the probability that a sample would have a test statistics at least as extreme as the one observed, if these assumptions (and thus the null hypothesis) were true. This probability is called the **p-value**. \n",
    "\n",
    "To draw conclusions, we use a predetermined cutoff probability called the level of significance $\\alpha$ (typically 5%). \n",
    "\n",
    "+ $\\text{p-value }\\leq\\alpha$: the observed data is very unlikely under the null hypothesis so we reject it. The observed effect is statistically significant.\n",
    "+ $\\text{p-value }\\gt\\alpha$: we fail to reject the null hypothesis. The observed effect is not statistically significant.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "___\n",
    "\n",
    "# PROBABILITY OF FALSE INFERENCE\n",
    "\n",
    "A statistical test has several important parameters to indicate acceptable probabilities of false inference:\n",
    "+ alpha (Type I Error): probability of false positive.\n",
    "+ beta (Type II Error): probability of false negative.\n",
    "+ power (1 - beta): probability of true negative (correcting failing to reject the null hypothesis).\n",
    "\n",
    "These parameters are linked to:\n",
    "+ effect size: the stronger an effect, the easier it is to correctly infer it.\n",
    "+ sample size: the larger the sample, the easier it is to capture even small effects.\n",
    "\n",
    "\n",
    "## Types of Errors\n",
    "\n",
    "There are four possible outcomes for our hypothesis testing, with two [types of errors](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors):\n",
    "\n",
    "| Decision          | $$H_0$$ is True                      | $$H_0$$ is False                     |\n",
    "|-------------------:|:---------------------------------:|:---------------------------------:|\n",
    "| **Reject H0** | **Type I error**: False Positive   | Correct inference: True Positive |\n",
    "| **Fail to reject H0** | Correct inference: True Negative | **Type II error**: False Negative |\n",
    "\n",
    "_Note: Decreasing the Type I error increases the probability of the Type II error._\n",
    "\n",
    "\n",
    "### Type I error\n",
    "\n",
    "The Type I error is the probability of incorrecly rejecting the null hypothesis when the sample belongs to the population but with extreme values; this probability is equal to the level of significance $\\alpha$. It is also called False Positive: falsely stating that the alternate hypothesis is true.\n",
    "\n",
    "\n",
    "### Type II error\n",
    "\n",
    "The Type II error $\\beta$ is the probability of incorrectly failing to reject a null hypothesis; it is also called False Negative.\n",
    "\n",
    "\n",
    "## Statistical Power\n",
    "\n",
    "[Power](https://en.wikipedia.org/wiki/Statistical_power), also called sensitivity, is the probability of correctly rejecting a false $H_0$; It is equal to $1 - \\beta$.\n",
    "\n",
    "Calculating the power helps asserting whether we can confidently fail to reject the null hypothesis (when the p-values is large). A low power (typically less than 80%) means that a real but small effect might not be detected in the conditions of our test.\n",
    "\n",
    "This [article](https://www.statisticsteacher.org/2017/09/15/what-is-power/) provides fictional studies examples that illustrate the need for careful experimental design.\n",
    "\n",
    "\n",
    "## P-Value vs Errors\n",
    "\n",
    "The p-value is linked to both error types:\n",
    "\n",
    "+ alpha is the maximum p-value we consider low enough to safely reject $H_0$.\n",
    "+ power is the probability we correctly fail to reject $H_0$ when the p-value is above alpha.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "___\n",
    "\n",
    "# EFFECT SIZE\n",
    "\n",
    "The [effect size](https://en.wikipedia.org/wiki/Effect_size) is a number that measures the strength of the relationship between two variables. Examples of effect sizes include the correlation between two variables, the regression coefficient in a regression, the mean difference, or the risk of a particular event (such as a heart attack) happening. \n",
    "\n",
    "\n",
    "## Difference Between Means\n",
    "\n",
    "An effect size based on means usually considers the standardized mean difference between two populations or samples. For instance, [Cohen's d](https://en.wikipedia.org/wiki/Effect_size#Cohen's_d) is the difference of means divided by pooled standard deviation. The smaller its value, the smaller the effect size.\n",
    "\n",
    "\n",
    "## Correlation & Variance Explained\n",
    "\n",
    "These effect sizes estimate the amount of the variance within an experiment that is \"explained\" or \"accounted for\" by the experiment's model. \n",
    "\n",
    "The [Pearson's Correlation Coefficient](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient) is the covariance of two variables, divided by the product of their standard deviations. Its values can vary in magnitude from −1 to 1, with −1 indicating a perfect negative linear relation, 1 indicating a perfect positive linear relation, and 0 indicating no linear relation between two variables.\n",
    "\n",
    "[Cohen's $f^2$](https://en.wikipedia.org/wiki/Effect_size#Cohen's_%C6%922) is one of several effect size measures used in the context of an F-test for ANOVA or multiple regression.\n",
    "\n",
    "\n",
    "## Categorical variables\n",
    "\n",
    "Commonly used measures of association for categorical variables are the Phi coefficient and [Cohen's w](https://en.wikipedia.org/wiki/Effect_size#Cohen's_w). Another useful measure is the [Odds Ratio](https://en.wikipedia.org/wiki/Odds_ratio).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "___\n",
    "\n",
    "# SAMPLE SIZE\n",
    "## Choosing the right Sample Size\n",
    "\n",
    "Most statistical tests can assess the value of one parameter among alpha, power, normalized effect size and sample size, assuming the other three are known. This means that we can determine the proper [sample size](https://en.wikipedia.org/wiki/Sample_size_determination) required to perform a sound experiment, before actually collecting any data. We'll cover this in more details in the next chapters. \n",
    "\n",
    "\n",
    "## Large Samples\n",
    "\n",
    "Tests become more sensitive with large sample sizes: even minuscule effects can become statistically significant. This is why it is very important to measure the effect size for large samples:\n",
    "\n",
    "> The question is not whether differences are ‘significant’ (they nearly always are in large samples), but whether they are interesting. Forget statistical significance, what is the practical significance of the results?\n",
    "\n",
    "More details can be found in this [academic paper](https://pdfs.semanticscholar.org/262b/854628d8e2b073816935d82b5095e1703977.pdf/).\n",
    "\n",
    "More useful links:\n",
    "+ https://statmodeling.stat.columbia.edu/2009/06/18/the_sample_size/\n",
    "+ https://stats.stackexchange.com/questions/2516/are-large-data-sets-inappropriate-for-hypothesis-testing\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}