{
 "cells": [
  {
   "source": [
    "# SUMMARY STATISTICS\n",
    "## Sample Statistics\n",
    "\n",
    "As mentioned earlier, and as every generalization we'll make, the value of the parameters of interest will be infered from the sample data; more specifically, we will use the values of the corresponding sample **statistics**. Which ones we use depend on the type of variable:\n",
    "\n",
    "![png](../../img/stat_tests/inference.png)\n",
    "\n",
    "A few important limitations:\n",
    "+ a sample is only part of the population; the numerical value of its statistic will not be the exact value of the parameter.\n",
    "+ the observed value of the statistic depends on the selected sample.\n",
    "+ some variability in the values of a statistic, over different samples, is unavoidable.\n",
    "\n",
    "\n",
    "## Maximum Likelihood Estimate\n",
    "\n",
    "The [**Maximum Likelihood Estimator**](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) is the value of the parameter space (i.e. the set of all values the parameter can take) that is the **most likely** to have **generated our sample**. \n",
    "\n",
    "+ for discrete distributions, the MLE of the probability of success is equal to successes / total trials.\n",
    "+ for continuous distributions:\n",
    "    + the MLE of the population mean is the sample mean. \n",
    "    + the MLE of the population variance is the sample variance.\n",
    "\n",
    "As the sample size increases, the MLE converges towards the true value of the population parameter.\n",
    "\n",
    "_Note: in more complex problems, the MLE can only be found via numerical optimization._\n",
    "\n",
    "\n",
    "## Sampling Distribution of Sample Statistics\n",
    "\n",
    "The true value of a population's parameter is usually unknown; we try to estimate it based on the available sample data. If we repeated the sampling process multiple times, we would have obtained slightly different values of the sample statistics. This means that the sample statistic is a random variable; it has a  **sampling distribution** we can study. \n",
    "\n",
    "Due to the random nature of sampling, some samples are not representative of the population. It means that a small proportion of samples, typically noted $\\alpha$, will produce incorrect inferences. This probability of errors can be controlled to build **(1 - $\\alpha$) Percent Confidence Intervals**. \n",
    "\n",
    "This means that for (1 - $\\alpha$) percents of all samples, the calculated interval will actually include the parameter value.\n",
    "\n",
    "_Note: this is not a probability. The interval either includes the parameter value or it doesn't._\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "___\n",
    "\n",
    "#  CENTRAL LIMIT THEOREM\n",
    "## CLT and Sample Mean\n",
    "\n",
    "Let ${X_{1},\\ldots ,X_{n}}$ be a sequence of independent and identically distributed (i.i.d.) random variables drawn from a distribution of expected value $\\mu$ and finite variance $\\sigma^2$. Let ${\\bar {X}}_{n}$ be the sample average: ${\\bar {X}}_{n} = ({X_{1} + \\ldots + X_{n}}) / n$.\n",
    "\n",
    "The [Law of Large Numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers) states that the sample mean converges to $\\mu$ as the sample size increases.\n",
    "\n",
    "The [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem) states that during this convergence, the difference between the sample mean and its limit $\\mu$ approximates the normal distribution with mean 0 and variance $\\sigma ^{2}/n$. A very important property of the CLT is that it holds regardless of the distribution of $X_i$ _(see example below)_.\n",
    "\n",
    "This means that for large samples (typically $n$ greater than 30), the sampling distribution of the sample mean is approximately normal and has the following parameters:\n",
    "\n",
    "+ mean: $\\mu$\n",
    "+ standard deviation (called **standard error**): $\\sigma / \\sqrt{n}$\n",
    "\n",
    "\n",
    "The example below illustrates the validity of the CLT even for distributions that are far from normal, provided the sample is large enough:\n",
    "\n",
    "![missing](../../img/exp-clt.png)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## CLT and Sample Variance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "## Z-Scores\n",
    "\n",
    "We can use two factors to assess the probability of observing the experimental results under the null hypothesis:\n",
    "+ The [**Z-score**](https://en.wikipedia.org/wiki/Standard_score) represents the number of standard deviations an observation is from the mean.\n",
    "+ The sampling distribution of sample statistic is centered around the population parameter and has a standard error linked to the population variance. \n",
    "\n",
    "It means that we can calculate the z-score of our sample statistic to calculate its p-value.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "___\n",
    "\n",
    "# POPULATION MEAN INFERENCE\n",
    "## CLT Confidence Interval\n",
    "\n",
    "As shown in the example, the sample means can take a large range of values; some are quite far from the actual population mean. We usually have only one sample to study the population, with no way of knowing where our sample mean sits in the sampling distribution. What we can do is leverage the CLT to quantify this uncertainty and build a [confidence interval](https://en.wikipedia.org/wiki/Confidence_interval) for the population mean. \n",
    "\n",
    "Given a confidence percentage $1 - \\alpha$, we can calculate the interval of values inside which $1 - \\alpha$ percent of all samples means will fall. A small percentage $\\alpha$ of all samples, the ones least representative of the population, will have a sample mean so far from the actual mean that it falls outside of this interval. \n",
    "\n",
    "Assuming our sample mean is indeed out of these extreme values, we have according to the CLT: $\\mu - z_{\\alpha/2} * se < \\bar{X} < \\mu + z_{1-\\alpha/2} * se$. The normal distribution is symmetrical, so $z_{\\alpha/2} = - z_{1 - \\alpha/2}$. It follows that:\n",
    "\n",
    "$$\\mu \\in \\bar {X} \\pm z_{1-\\alpha/2} \\frac{{\\sigma }}{\\sqrt {n}}$$\n",
    "\n",
    "Where $z_{1 - \\alpha/2}$ is the Z-score above which lay $\\alpha/2$ percent of all observations in a standard normal distribution.\n",
    "\n",
    "As an example, 95% of all sample means are between -2 and +2 standard errors from the sampling distribution mean, which is the population mean $\\mu$. It follows that for 95% of all the samples that could have been drawn from the population, the population mean is less than two standard errors away from the sample mean.\n",
    "\n",
    "_Note: we have no way of knowing if our sample is part of these 95%; for $\\alpha =$ 5% of all the possible samples, the confidence interval will not include $\\mu$ and the inference will be incorrect. This is why $\\alpha$ is called the Type I Error._\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Limits of the CLT\n",
    "\n",
    "The CLT Confidence intervals **do not works** when either:\n",
    "\n",
    "+ $\\sigma$ is unknown.\n",
    "+ the sample size $n$ is small.\n",
    "\n",
    "The [Student’s t distribution](https://en.wikipedia.org/wiki/Student's_t-distribution) is used instead. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "___ \n",
    "# POPULATION MEAN INFERENCE - T-DISTRIBUTION\n",
    "\n",
    "## Assumptions of the t-distribution\n",
    "\n",
    "The sampling distribution of the sample mean has to be roughly normal for the t-distribution to work well. It means that either:\n",
    "\n",
    "+ the population is normally distributed, even for small samples.\n",
    "+ the sample is large, regardless of the underlying distribution of data, thanks to the CLT.\n",
    "\n",
    "If the sample size is very small, we can use normal probability plots to check whether the sample may come from a normal distribution. \n",
    "\n",
    "_Note: When the sample size is large (30+ observations), the Student Distribution becomes extremely close to the normal distribution._\n",
    "\n",
    "### Example for the Exponential distribution\n",
    "\n",
    "Back to the Exponential Distribution, the figure below shows the experimental sample mean distribution vs T vs Normal for different sample sizes: 2, 5, 10 and 20:\n",
    "+ the t-distribution gets close to normal even for relatively small sample sizes. \n",
    "+ it does not approximate the empirical distribution very well for smaller sample sizes because its assumptions are not met: the exponential distribution is far from normal.\n",
    "\n",
    "<img class=\"center-block\" src=\"https://sebastienplat.s3.amazonaws.com/dc954f3e9562d53b7829a2adcd2854ff1490011103173\"/>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Properties of the t-distribution\n",
    "\n",
    "Small samples are more likely to underestimate $\\sigma$ and have a mean that differs from $\\mu$. The t-distribution accounts for this uncertainty with heavier tails compared to a Gaussian: the probability of extreme values becomes comparatively higher. This means its confidence intervals are wider than CLT ones for the same confidence level.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Mathematical Notions\n",
    "\n",
    "We have seen that under the assumptions of the CLT:\n",
    "\n",
    "$$\\frac { \\bar {X_n} - \\mu }{\\sigma /\\sqrt {n}} \\sim N(0, 1)$$\n",
    "\n",
    "Under the assumptions of the t-distribution, we can substitute the unbiased sample variance $\\widehat {\\sigma}^2$. In this case, the sampling distribution of the sample mean follows a t-distribution with $n-1$ degrees of freedom ([mathematical proof](https://www.math.arizona.edu/~jwatkins/ttest.pdf)):\n",
    "\n",
    "$$\\frac { \\bar {X_n} - \\mu }{\\widehat {\\sigma} /\\sqrt {n}} \\sim t_{n-1}$$\n",
    "\n",
    "_Note: the [unbiased](https://dawenl.github.io/files/mle_biased.pdf) variance calculated from a sample of size $n$ uses $n-1$ to average the distances from the mean, in what is called the [Bessel's correction](https://en.wikipedia.org/wiki/Bessel%27s_correction)._\n",
    "\n",
    "_Note: formally, the t-distribution comes from the division of a normal distribution by a $\\chi^2$ distribution. This is the case here: the CLT states that the sampling distribution of the sample mean is asymptotally normal and the sampling distribution of the sample variance is asymptotally  $\\chi^2$._\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Confidence intervals\n",
    "\n",
    "Similarly to what we saw for situations with known variance, the $1 - \\alpha$ T Confidence Interval is:\n",
    "\n",
    "$$\\bar{y} \\pm T_{\\alpha/2, n-1} \\times \\widehat {\\sigma}^2 / \\sqrt{n}$$\n",
    "\n",
    "Where $T_{\\alpha/2, n-1}$ is the distance from the mean of the t-distribution with n-1 degrees of freedom above which lay $\\alpha/2$ percent of all observations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Example\n",
    "\n",
    "We can calculate the confidence interval for the mean petal length of three species of iris, based on their respective samples. The sample size is n=50, which is large enough that we can use the t-distribution.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "setosa    : median: 1.50 - mean: 1.46 - se: 0.0243 - mean 95% CI: [1.413, 1.511]\nversicolor: median: 4.35 - mean: 4.26 - se: 0.0658 - mean 95% CI: [4.128, 4.392]\nvirginica : median: 5.55 - mean: 5.55 - se: 0.0773 - mean 95% CI: [5.397, 5.707]\n"
     ]
    }
   ],
   "source": [
    "# modules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from sklearn import datasets\n",
    "sns.set()\n",
    "\n",
    "# load & format iris dataset\n",
    "iris = datasets.load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
    "iris_df['species'] = [iris.target_names[i] for i in iris.target]\n",
    "\n",
    "# petal lenghts of each species\n",
    "setosa_petal_length = iris_df.loc[iris_df['species'] == 'setosa', 'petal length (cm)'].to_numpy()\n",
    "versicolor_petal_length = iris_df.loc[iris_df['species'] == 'versicolor', 'petal length (cm)'].to_numpy()\n",
    "virginica_petal_length = iris_df.loc[iris_df['species'] == 'virginica', 'petal length (cm)'].to_numpy()\n",
    "\n",
    "# sample statistics for iris species\n",
    "for species, petal_length in zip(['setosa', 'versicolor', 'virginica'], [setosa_petal_length, versicolor_petal_length, virginica_petal_length]):\n",
    "\n",
    "    # sample stats\n",
    "    size = len(petal_length)\n",
    "    mean = np.mean(petal_length)\n",
    "    var = np.var(petal_length)\n",
    "    se = np.sqrt(var/size)\n",
    "    median = np.median(petal_length)\n",
    "\n",
    "    # CI for pop mean\n",
    "    tdist = stats.t(df=size-1, loc=mean, scale=se)\n",
    "    interval = tdist.interval(0.95)\n",
    "    \n",
    "    print('{: <10}: median: {:.2f} - mean: {:0.3} - se: {:0.3} - mean 95% CI: [{:0.3f}, {:0.3f}]'.format(species, median, mean, se, interval[0], interval[1]))\n"
   ]
  },
  {
   "source": [
    "![iris-ci](../../img/iris-ci.png)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "___\n",
    "\n",
    "# POPULATION QUANTILE INFERENCE\n",
    "## Confidence Interval\n",
    "\n",
    "In cases where the distribution of the variable of interest is not unimodal and symmetrical, it is more useful to focus on percentiles (median and IQR). We make no assumptions about the shape of the distribution and only use order statistics to build the confidence intervals for the population percentiles. For a given sample size $n$, the confidence interval for the quantile $Q$ is ([mathematical proof](https://stats.stackexchange.com/questions/122001/confidence-intervals-for-median)):\n",
    "\n",
    "$$P(X_l \\leq Q \\leq X_u) \\geq binom(n, q).cdf(u) - binom(n, q).cdf(l)$$\n",
    "\n",
    "For a given confidence level $1 - \\alpha$, the goal is to find $l \\leq u$ so that:\n",
    "\n",
    "$$binom(n, q).cdf(u) - binom(n, q).cdf(l) \\geq 1 - \\alpha$$\n",
    "\n",
    "The resulting confidence interval will have a coverage of at least coverage at least $1 - \\alpha$.\n",
    "\n",
    "_Note: the CI will be larger than if you had made some stronger assumptions about the shape of the distribution._\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## IQR coverage\n",
    "\n",
    "It follows that we can find the values of $l \\leq u$ that will allow us to build the confidence interval of the three quartiles of the IQR:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.056 0.244 0.526 0.776 0.922 0.98  0.996 1.    1.    1.   ]\n[0.001 0.011 0.055 0.172 0.377 0.623 0.828 0.945 0.989 0.999]\n[0.    0.    0.    0.004 0.02  0.078 0.224 0.474 0.756 0.944]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# parameters\n",
    "sample_size = 10\n",
    "quantile = 0.5\n",
    "\n",
    "print(np.round(stats.binom.cdf(range(sample_size), sample_size, 0.25), 3)) # condition approx. met for 0-4 (0.87)\n",
    "print(np.round(stats.binom.cdf(range(sample_size), sample_size, 0.50), 3)) # condition approx. met for 2-7 (0.89)\n",
    "print(np.round(stats.binom.cdf(range(sample_size), sample_size, 0.75), 3)) # condition approx. met for 5-9 (0.87)\n"
   ]
  },
  {
   "source": [
    "## Examples\n",
    "\n",
    "We can check the actual coverage of these confidence intervals for well-known distributions:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "coverage for normal:  Q1 = 0.871, Q2 = 0.888, Q3 = 0.857\ncoverage for poisson: Q1 = 0.989, Q2 = 0.985, Q3 = 0.964\n"
     ]
    }
   ],
   "source": [
    "# modules\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# check coverage\n",
    "def covers(pop_dist, quantile, lower_idx, upper_idx, sample_size=10, n_samples=1000):\n",
    "    \"\"\"returns the percentage of samples for which\n",
    "       the quantile of the population distribution\n",
    "       is actually in (sample[lower_idx], sample[upper_idx])\"\"\"\n",
    "\n",
    "    # samples to assess\n",
    "    samples = np.reshape(pop_dist.rvs(size=sample_size*n_samples), (sample_size, -1))\n",
    "    samples.sort(axis=0)\n",
    "    \n",
    "    # percentage of samples that actually include the quantile\n",
    "    quantile = pop_dist.ppf(quantile)\n",
    "    in_ci = np.mean((samples[lower_idx, ] <= quantile) & (samples[upper_idx, ] >= quantile))\n",
    "    return in_ci\n",
    "\n",
    "# IQR for the normal dist (parameters for sample_size=10)\n",
    "norm_Q1 = covers(stats.norm, 0.25, 0, 4)\n",
    "norm_Q2 = covers(stats.norm, 0.50, 2, 7)\n",
    "norm_Q3 = covers(stats.norm, 0.75, 5, 9)\n",
    "print('coverage for normal:  Q1 = {:.3f}, Q2 = {:.3f}, Q3 = {:.3f}'.format(norm_Q1, norm_Q2, norm_Q3))\n",
    "\n",
    "# IQR for the poisson dist (parameters for sample_size=10)\n",
    "poisson_Q1 = covers(stats.poisson(mu=2), 0.25, 0, 4)\n",
    "poisson_Q2 = covers(stats.poisson(mu=2), 0.50, 2, 7)\n",
    "poisson_Q3 = covers(stats.poisson(mu=2), 0.75, 5, 9)\n",
    "print('coverage for poisson: Q1 = {:.3f}, Q2 = {:.3f}, Q3 = {:.3f}'.format(poisson_Q1, poisson_Q2, poisson_Q3))\n"
   ]
  },
  {
   "source": [
    "___\n",
    "\n",
    "# POPULATION PROPORTION INFERENCE\n",
    "\n",
    "A population proportion $P$ is the percentage of the population that has some characteristic of interest. It can be defined as follows: $P = X / N$, where X is the count of successes in the population (i.e. individuals with the characteristic) and N its size.\n",
    "\n",
    "The definition is the same for a sample proportion: $\\hat {p} = x/n$, where $x$ is the count of successes in the sample and $n$ its size.\n",
    "\n",
    "The sampling distribution of the sampling proportion is a binomial distribution with parameters $n$ and $P$, a discrete probability distribution that is difficult to calculate for large samples. A [variety of approximations](https://www.ucl.ac.uk/english-usage/staff/sean/resources/binomialpoisson.pdf) are used to calculate the confidence interval for $P$, all with their own tradeoffs in coverage (i.e. the percentage of samples for which the confidence interval actually includes $P$) and computational intensity. This [article](https://towardsdatascience.com/five-confidence-intervals-for-proportions-that-you-should-know-about-7ff5484c024f) illustrates these tradeoffs very well.\n",
    "\n",
    "## Wald Interval\n",
    "\n",
    "The Wald Interval leverages the fact that the binomial distribution can be approximated by a normal distribution for large samples. Using  $\\mu = n * \\hat {p}$  and ${\\sigma}^2 = n * \\hat {p} (1 - \\hat {p})$, it follows that for a confidence level $1 - \\alpha$, the confidence interval for $P$ is:\n",
    "\n",
    "$$ P = \\hat {p} \\pm z_{1-\\alpha/2} \\times \\sqrt{\\frac{\\hat {p} (1 - \\hat {p})}{n}}$$\n",
    "\n",
    "This formula doesn't work well for small samples because the normal approximation doesn't hold. It also fails when $\\hat {p}$ is not close to 0.5 because the normal approximation is unconstrained whereas $P$ cannot, for obvious reasons, exceed the range \\[0, 1\\]. This means that many possible values of the approximation fall outside of the possible values of $P$.\n",
    "\n",
    "\n",
    "## Wilson's Score Interval\n",
    "\n",
    "The Wilson's Score Interval keeps the binomial approximation but uses $\\mu = n * P$  and ${\\sigma}^2 = n * P (1 - P)$. This leads to a quadratic equation for $P$ that translates into an assymetric confidence interval centered around the weighted average of $\\hat {p}$ and $1/2$, with $\\hat {p}$ receiving greater weight as the sample size increases.\n",
    "\n",
    "_Note: the Wilson's Score Interval can also receive a continuity correction called the Yate’s continuity correction. It is recommended for small samples (n < 40) or when $\\hat{p}$ is close to either 0 or 1._\n",
    "\n",
    "\n",
    "## Other methods\n",
    "\n",
    "There are [several other methods](https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval) used to calculate the confidence interval for proportions. The most commonly used are:\n",
    "+ The Clopper-Pearson method uses no approximation and therefore always has the expected coverage, but it is fairly conservative: its confidence intervals are quite large. \n",
    "+ The Agresti–Coull Interval, expressed in its simplest form for the 95% confidence level, just adds two successes and two failures to the sample data and works well for large samples.\n",
    " \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Example\n",
    "\n",
    "Suppose a presidential election is taking place in a democracy. A political scientist wants to determine what percentage of the voter population support candidate B. She polls a random sample of 400 eligible voters.\n",
    "\n",
    "Let's assume the poll shows that 272 voters support candidate B. The sample size is large enough and the sample proportion 0.68 is close enough to 0.68 that the normal approximation required by the Wald Interval to work properly are met. The four methods yield very similar confidence intervals.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the most common methods are implemented in statsmodel: {‘normal’, ‘agresti_coull’, ‘beta’, ‘wilson’, ‘binom_test’}\n",
    "# note that the implementation of Clopper-Pearson is called beta because it uses the quantiles of the beta distribution\n",
    "# note that the implementation of Wilson doesn't currently support continuity corrections\n",
    "stats.proportion.proportion_confint(count, nobs, alpha=0.05, method='normal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "normal       : [0.634, 0.726]\nwilson       : [0.633, 0.724]\nbeta         : [0.632, 0.725]\nagresti_coull: [0.633, 0.724]\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "for method in ['normal', 'wilson', 'beta', 'agresti_coull']:\n",
    "    ci = sm.stats.proportion_confint(272, 400, alpha=0.05, method=method)\n",
    "    print('{:<13}: [{:.3f}, {:.3f}]'.format(method, ci[0], ci[1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 32-bit",
   "name": "python38032bit64a64ed7a47843b8be3706a54e9a0958",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}