{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this programming assignment we will illustrate a very severe data leakage, that can often be found in competitions, where the pairs of object should be scored, e.g. predict $1$ if two objects belong to the same class and $0$ otherwise. \n",
    "\n",
    "The data in this assignment is taken from a real competition, and the funniest thing is that *we will not use training set at all* and achieve almost 100% accuracy score! We will just exploit the leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import scipy.sparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Let's load the test data. This dataset lists many pairs of IDs that represent images of various classes. Our goal is to predict which pairs of IDs have the same class.\n",
    "\n",
    "We don't have any training data here, nor do we have any features for the test objects. All we need to solve this task is the file with the indices for the pairs that we need to compare.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./test_pairs.csv')\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.shape)"
   ]
  },
  {
   "source": [
    "## Pairs\n",
    "\n",
    "We have 26,325 IDs, but not all possible pairs are in the test set. Otherwise we would have 26,325 * 26,324 = 69,2979,300 pairs. Is it possible that the pairing wasn't random?\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = test['FirstId'].to_list() + test['SecondId'].to_list()\n",
    "unique_ids = np.array(unique_ids, dtype=int)\n",
    "print('distinct IDs: {} - from {} to {}'.format(np.unique(unique_ids).shape[0], unique_ids.min(), unique_ids.max()))\n"
   ]
  },
  {
   "source": [
    "After submitting the dummy predictions below, we get a score of 0.5. This means that exactly half of the pairs are coming from the same class: the organizers wanted to create a balanced dataset, which means the pairing is not random. If we can figure out the methodology used, we will get a significant boost in scoring.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy predictions\n",
    "one_preds = test[['pairId']].copy(deep=True)\n",
    "one_preds['Prediction'] = 1\n",
    "one_preds.to_csv('./test_one_preds.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occurrences\n",
    "\n",
    "Some IDs appear more in the first column than others. But the unique IDs are distributed evenly across the range of appeareances. The conclusion is similar for the second column, but with less consistency. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of images per number of occurrences in the FirstId column\n",
    "test['FirstId'].value_counts().value_counts().sort_index().to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of images per number of occurrences in the SecondId column\n",
    "test['SecondId'].value_counts().value_counts().sort_index().to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of appeareances in the first and second column are correlated\n",
    "counts = test['FirstId'].value_counts().to_frame().join(test['SecondId'].value_counts())\n",
    "counts['count'] = 1\n",
    "counts.groupby(['FirstId', 'SecondId']).count().T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "All IDs split into two buckets: the ones that appear 21 times and the ones that appear 36 times.\n",
    "+ the max split for the ones appearing 21 times is 7-14.\n",
    "+ the max split for the ones appearing 36 times is 15-21.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# occurences\n",
    "occurences = pd.Series(test['FirstId'].to_list() + test['SecondId'].to_list(), name='agg')\n",
    "display(occurences.value_counts().value_counts())\n",
    "display(occurences.value_counts().value_counts() / 1755)\n"
   ]
  },
  {
   "source": [
    "## Pairs of images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip df to get all pairs (x,y) and (y,x)\n",
    "test_flip = test[['SecondId', 'FirstId']]\n",
    "test_flip.columns = ['FirstId', 'SecondId']\n",
    "\n",
    "# merge flipped df with initial df to get all pairs; remove dupes\n",
    "test_pairs = pd.concat((test[['FirstId', 'SecondId']], test_flip), ignore_index=True).drop_duplicates()\n",
    "\n",
    "# total number of unique pairs\n",
    "print(test_pairs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we remove dupes, we almost get the same picture (with some rare edge cases)\n",
    "test_pairs['FirstId'].value_counts().value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this takes a long while to run\n",
    "\n",
    "# list\n",
    "counts_pairs = []\n",
    "\n",
    "# starting ID\n",
    "starting_ID = 0\n",
    "\n",
    "for starting_ID in range(26325):\n",
    "    # IDs directly linked to a given ID\n",
    "    idX = test_pairs.loc[test_pairs['FirstId']==starting_ID, 'SecondId'].to_list() + [starting_ID]\n",
    "\n",
    "    # IDs indirectly linked to starting_ID\n",
    "    idX_graph = test_pairs.loc[test_pairs['FirstId'].isin(idX)]\n",
    "    idX_graph_list = np.unique(np.array(idX_graph['FirstId'].tolist() + idX_graph['SecondId'].tolist()))\n",
    "\n",
    "    # pairs & unique IDs\n",
    "    counts_pairs.append((idX_graph.shape[0], idX_graph_list.shape[0]))\n",
    "\n",
    "counts_pairs_id = pd.DataFrame(counts_pairs, columns=['pairs', 'IDs'])\n"
   ]
  },
  {
   "source": [
    "## Finding patterns in the pairing process\n",
    "\n",
    "We will try to find patterns in the sampling method by using an [incidence matrix](https://en.wikipedia.org/wiki/Incidence_matrix), treating pairs `(FirstId, SecondId)` as edges in an undirected graph. \n",
    "\n",
    "_Note: incidence matrices are typically very sparse with huge dimensions, so it's best to use specific tools like `scipy.sparse` to manipulate them. More information can be found in [wikipedia](https://en.wikipedia.org/wiki/Sparse_matrix) and the [scipy.sparse reference](https://docs.scipy.org/doc/scipy/reference/sparse.html)._\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows & cols (scipy goes crazy if a matrix is indexed with pandas' series, so we transform them into numpy arays)\n",
    "rows = test_pairs['FirstId'].to_numpy()\n",
    "cols = test_pairs['SecondId'].to_numpy()\n",
    "\n",
    "# sparse matrix\n",
    "inc_mat = scipy.sparse.coo_matrix(([1] * len(rows), (rows, cols)))\n",
    "\n",
    "# Sanity checks\n",
    "assert inc_mat.max() == 1\n",
    "assert inc_mat.sum() == 736872\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mathematical operations are faster in csr format\n",
    "inc_mat_csr = inc_mat.tocsr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot pairs\n",
    "#fig, ax = plt.subplots(figsize=(22,22))\n",
    "#plt.spy(inc_mat_csr, markersize=0.1)\n"
   ]
  },
  {
   "source": [
    "## Measure how close pairing lists are \n",
    "\n",
    "Each row of the incidence matrix represents the pairs that exist for a given image. We can measure how close two IDs are from one another by comparing how similar their representation is, i.e. how similar their pairings are.\n",
    "\n",
    "For each pair of images, we compare how much overlap exists between their pairing list. We use the dot product to do so.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# representations of all IDs\n",
    "rows_FirstId  = inc_mat_csr[test['FirstId'].to_numpy()]\n",
    "rows_SecondId  = inc_mat_csr[test['SecondId'].to_numpy()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# measure the overlap between the pairing list of each pair\n",
    "f = np.array(rows_FirstId.multiply(rows_SecondId).sum(axis=1)).flatten()\n",
    "\n",
    "# Sanity check\n",
    "assert f.shape == (368550, )\n"
   ]
  },
  {
   "source": [
    "## Convert to binary predictions\n",
    "\n",
    "We know that half the pairs belong to the same class. We will find the threshold that splits our predictions into equal buckets.\n",
    "\n",
    "_Note: the more overlap, the likelier it is that two images belong to the same class._\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# frequencies of each number of shared comparison IDs \n",
    "unique, counts = np.unique(f, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we choose the value that splits the dataset in roughly equal buckets\n",
    "pred = f >= 20\n",
    "\n",
    "# final predictions\n",
    "submission = test.loc[:,['pairId']]\n",
    "submission['Prediction'] = pred.astype(int)\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "source": [
    "Go to the [assignment page](https://www.coursera.org/learn/competitive-data-science/programming/KsASv/data-leakages/submission) and submit your `.csv` file in 'My submission' tab.\n",
    "\n",
    "If you did everything right, the score should be very high."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "Interestingly, it is not the only leak in this dataset. There is another totally different way to get almost 100% accuracy. Try to find it!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "8307c6e89996e69277c298e8713cb9b95874798da9a4c0beafd992c24fca3d7a"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}