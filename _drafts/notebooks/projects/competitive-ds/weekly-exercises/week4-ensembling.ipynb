{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check your versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.13.1\n",
      "pandas 0.20.3\n",
      "scipy 0.19.1\n",
      "sklearn 0.19.0\n",
      "lightgbm 2.0.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn\n",
    "import scipy.sparse \n",
    "import lightgbm \n",
    "\n",
    "for p in [np, pd, scipy, sklearn, lightgbm]:\n",
    "    print (p.__name__, p.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important!** There is a huge chance that the assignment will be impossible to pass if the versions of `lighgbm` and `scikit-learn` are wrong. The versions being tested:\n",
    "\n",
    "    numpy 1.13.1\n",
    "    pandas 0.20.3\n",
    "    scipy 0.19.1\n",
    "    sklearn 0.19.0\n",
    "    ligthgbm 2.0.6\n",
    "    \n",
    "\n",
    "To install an older version of `lighgbm` you may use the following command:\n",
    "```\n",
    "pip uninstall lightgbm\n",
    "pip install lightgbm==2.0.6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this programming assignment you are asked to implement two ensembling schemes: simple linear mix and stacking.\n",
    "\n",
    "We will spend several cells to load data and create feature matrix, you can scroll down this part or try to understand what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "pd.set_option('display.max_rows', 600)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data from the hard drive first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales = pd.read_csv('../readonly/final_project_data/sales_train.csv.gz')\n",
    "shops = pd.read_csv('../readonly/final_project_data/shops.csv')\n",
    "items = pd.read_csv('../readonly/final_project_data/items.csv')\n",
    "item_cats = pd.read_csv('../readonly/final_project_data/item_categories.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use only 3 shops for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales = sales[sales['shop_id'].isin([26, 27, 28])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to prepare the features. This part is all implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pandas/core/groupby.py:4036: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\n",
      "  return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create \"grid\" with columns\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "# For every month we create a grid from all shops/items combinations from that month\n",
    "grid = [] \n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "\n",
    "# Turn the grid into a dataframe\n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "\n",
    "# Groupby data to get shop-item-month aggregates\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n",
    "# Fix column names\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n",
    "# Join it to the grid\n",
    "all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "# Same as above but with shop-month aggregates\n",
    "gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop':'sum'}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Same as above but with item-month aggregates\n",
    "gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_item':'sum'}})\n",
    "gb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Downcast dtypes from 64 to 32 bit to save memory\n",
    "all_data = downcast_dtypes(all_data)\n",
    "del grid, gb \n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a grid, we can calculate some features. We will use lags from [1, 2, 3, 4, 5, 12] months ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ad76dc238b49c8b18d5301800f8751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# List of columns that we will use to create lags\n",
    "cols_to_rename = list(all_data.columns.difference(index_cols)) \n",
    "\n",
    "shift_range = [1, 2, 3, 4, 5, 12]\n",
    "\n",
    "for month_shift in tqdm_notebook(shift_range):\n",
    "    train_shift = all_data[index_cols + cols_to_rename].copy()\n",
    "    \n",
    "    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "    \n",
    "    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "    train_shift = train_shift.rename(columns=foo)\n",
    "\n",
    "    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n",
    "\n",
    "del train_shift\n",
    "\n",
    "# Don't use old data from year 2013\n",
    "all_data = all_data[all_data['date_block_num'] >= 12] \n",
    "\n",
    "# List of all lagged features\n",
    "fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n",
    "# We will drop these at fitting stage\n",
    "to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n",
    "\n",
    "# Category for each item\n",
    "item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n",
    "\n",
    "all_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n",
    "all_data = downcast_dtypes(all_data)\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['target', 'target_item', 'target_shop', 'date_block_num']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_drop_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end, we've created a feature matrix. It is stored in `all_data` variable. Take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>target</th>\n",
       "      <th>target_shop</th>\n",
       "      <th>target_item</th>\n",
       "      <th>target_lag_1</th>\n",
       "      <th>target_item_lag_1</th>\n",
       "      <th>target_shop_lag_1</th>\n",
       "      <th>target_lag_2</th>\n",
       "      <th>target_item_lag_2</th>\n",
       "      <th>target_shop_lag_2</th>\n",
       "      <th>target_lag_3</th>\n",
       "      <th>target_item_lag_3</th>\n",
       "      <th>target_shop_lag_3</th>\n",
       "      <th>target_lag_4</th>\n",
       "      <th>target_item_lag_4</th>\n",
       "      <th>target_shop_lag_4</th>\n",
       "      <th>target_lag_5</th>\n",
       "      <th>target_item_lag_5</th>\n",
       "      <th>target_shop_lag_5</th>\n",
       "      <th>target_lag_12</th>\n",
       "      <th>target_item_lag_12</th>\n",
       "      <th>target_shop_lag_12</th>\n",
       "      <th>item_category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>10994</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6454.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>10992</td>\n",
       "      <td>12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7521.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>10991</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5609.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6753.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7521.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>10988</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6454.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5609.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6753.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>11002</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shop_id  item_id  date_block_num  target  target_shop  target_item  \\\n",
       "0       28    10994              12     1.0       6949.0          1.0   \n",
       "1       28    10992              12     3.0       6949.0          4.0   \n",
       "2       28    10991              12     1.0       6949.0          5.0   \n",
       "3       28    10988              12     1.0       6949.0          2.0   \n",
       "4       28    11002              12     1.0       6949.0          1.0   \n",
       "\n",
       "   target_lag_1  target_item_lag_1  target_shop_lag_1  target_lag_2  \\\n",
       "0           0.0                1.0             8499.0           0.0   \n",
       "1           3.0                7.0             8499.0           0.0   \n",
       "2           1.0                3.0             8499.0           0.0   \n",
       "3           2.0                5.0             8499.0           4.0   \n",
       "4           0.0                1.0             8499.0           0.0   \n",
       "\n",
       "   target_item_lag_2  target_shop_lag_2  target_lag_3  target_item_lag_3  \\\n",
       "0                1.0             6454.0           0.0                0.0   \n",
       "1                0.0                0.0           0.0                0.0   \n",
       "2                0.0                0.0           0.0                1.0   \n",
       "3                5.0             6454.0           5.0                6.0   \n",
       "4                0.0                0.0           0.0                0.0   \n",
       "\n",
       "   target_shop_lag_3  target_lag_4  target_item_lag_4  target_shop_lag_4  \\\n",
       "0                0.0           0.0                0.0                0.0   \n",
       "1                0.0           0.0                0.0                0.0   \n",
       "2             5609.0           0.0                2.0             6753.0   \n",
       "3             5609.0           0.0                2.0             6753.0   \n",
       "4                0.0           0.0                0.0                0.0   \n",
       "\n",
       "   target_lag_5  target_item_lag_5  target_shop_lag_5  target_lag_12  \\\n",
       "0           0.0                0.0                0.0            0.0   \n",
       "1           0.0                1.0             7521.0            0.0   \n",
       "2           2.0                4.0             7521.0            0.0   \n",
       "3           0.0                0.0                0.0            0.0   \n",
       "4           0.0                0.0                0.0            0.0   \n",
       "\n",
       "   target_item_lag_12  target_shop_lag_12  item_category_id  \n",
       "0                 0.0                 0.0                37  \n",
       "1                 0.0                 0.0                37  \n",
       "2                 0.0                 0.0                40  \n",
       "3                 0.0                 0.0                40  \n",
       "4                 0.0                 0.0                40  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a sake of the programming assignment, let's artificially split the data into train and test. We will treat last month data as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test `date_block_num` is 33\n"
     ]
    }
   ],
   "source": [
    "# Save `date_block_num`, as we can't use them as features, but will need them to split the dataset into parts \n",
    "dates = all_data['date_block_num']\n",
    "\n",
    "last_block = dates.max()\n",
    "print('Test `date_block_num` is %d' % last_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates_train = dates[dates <  last_block]\n",
    "dates_test  = dates[dates == last_block]\n",
    "\n",
    "X_train = all_data.loc[dates <  last_block].drop(to_drop_cols, axis=1)\n",
    "X_test =  all_data.loc[dates == last_block].drop(to_drop_cols, axis=1)\n",
    "\n",
    "y_train = all_data.loc[dates <  last_block, 'target'].values\n",
    "y_test =  all_data.loc[dates == last_block, 'target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>target_lag_1</th>\n",
       "      <th>target_item_lag_1</th>\n",
       "      <th>target_shop_lag_1</th>\n",
       "      <th>target_lag_2</th>\n",
       "      <th>target_item_lag_2</th>\n",
       "      <th>target_shop_lag_2</th>\n",
       "      <th>target_lag_3</th>\n",
       "      <th>target_item_lag_3</th>\n",
       "      <th>target_shop_lag_3</th>\n",
       "      <th>target_lag_4</th>\n",
       "      <th>target_item_lag_4</th>\n",
       "      <th>target_shop_lag_4</th>\n",
       "      <th>target_lag_5</th>\n",
       "      <th>target_item_lag_5</th>\n",
       "      <th>target_shop_lag_5</th>\n",
       "      <th>target_lag_12</th>\n",
       "      <th>target_item_lag_12</th>\n",
       "      <th>target_shop_lag_12</th>\n",
       "      <th>item_category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>10994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6454.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>10992</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7521.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>10991</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5609.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6753.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7521.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>10988</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6454.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5609.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6753.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>11002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shop_id  item_id  target_lag_1  target_item_lag_1  target_shop_lag_1  \\\n",
       "0       28    10994           0.0                1.0             8499.0   \n",
       "1       28    10992           3.0                7.0             8499.0   \n",
       "2       28    10991           1.0                3.0             8499.0   \n",
       "3       28    10988           2.0                5.0             8499.0   \n",
       "4       28    11002           0.0                1.0             8499.0   \n",
       "\n",
       "   target_lag_2  target_item_lag_2  target_shop_lag_2  target_lag_3  \\\n",
       "0           0.0                1.0             6454.0           0.0   \n",
       "1           0.0                0.0                0.0           0.0   \n",
       "2           0.0                0.0                0.0           0.0   \n",
       "3           4.0                5.0             6454.0           5.0   \n",
       "4           0.0                0.0                0.0           0.0   \n",
       "\n",
       "   target_item_lag_3  target_shop_lag_3  target_lag_4  target_item_lag_4  \\\n",
       "0                0.0                0.0           0.0                0.0   \n",
       "1                0.0                0.0           0.0                0.0   \n",
       "2                1.0             5609.0           0.0                2.0   \n",
       "3                6.0             5609.0           0.0                2.0   \n",
       "4                0.0                0.0           0.0                0.0   \n",
       "\n",
       "   target_shop_lag_4  target_lag_5  target_item_lag_5  target_shop_lag_5  \\\n",
       "0                0.0           0.0                0.0                0.0   \n",
       "1                0.0           0.0                1.0             7521.0   \n",
       "2             6753.0           2.0                4.0             7521.0   \n",
       "3             6753.0           0.0                0.0                0.0   \n",
       "4                0.0           0.0                0.0                0.0   \n",
       "\n",
       "   target_lag_12  target_item_lag_12  target_shop_lag_12  item_category_id  \n",
       "0            0.0                 0.0                 0.0                37  \n",
       "1            0.0                 0.0                 0.0                37  \n",
       "2            0.0                 0.0                 0.0                40  \n",
       "3            0.0                 0.0                 0.0                40  \n",
       "4            0.0                 0.0                 0.0                40  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First level models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to implement a basic stacking scheme. We have a time component here, so we will use ***scheme f)*** from the reading material. Recall, that we always use first level models to build two datasets: test meta-features and 2-nd level train-metafetures. Let's see how we get test meta-features first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test meta-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firts, we will run *linear regression* on numeric columns and get predictions for the last month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R-squared for linreg is 0.743180\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train.values, y_train)\n",
    "pred_lr = lr.predict(X_test.values)\n",
    "\n",
    "print('Test R-squared for linreg is %f' % r2_score(y_test, pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the we run *LightGBM*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R-squared for LightGBM is 0.738391\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "               'feature_fraction': 0.75,\n",
    "               'metric': 'rmse',\n",
    "               'nthread':1, \n",
    "               'min_data_in_leaf': 2**7, \n",
    "               'bagging_fraction': 0.75, \n",
    "               'learning_rate': 0.03, \n",
    "               'objective': 'mse', \n",
    "               'bagging_seed': 2**7, \n",
    "               'num_leaves': 2**7,\n",
    "               'bagging_freq':1,\n",
    "               'verbose':0 \n",
    "              }\n",
    "\n",
    "model = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100)\n",
    "pred_lgb = model.predict(X_test)\n",
    "\n",
    "print('Test R-squared for LightGBM is %f' % r2_score(y_test, pred_lgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, concatenate test predictions to get test meta-features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_level2 = np.c_[pred_lr, pred_lgb] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train meta-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now it is your turn to write the code**. You need to implement ***scheme f)*** from the reading material. Here, we will use duration **T** equal to month and **M=15**.  \n",
    "\n",
    "That is, you need to get predictions (meta-features) from *linear regression* and *LightGBM* for months 27, 28, 29, 30, 31, 32. Use the same parameters as in above models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# months to consider for level2 (K in M+K)\n",
    "months_level2 = [27, 28, 29, 30, 31, 32]\n",
    "\n",
    "# dates ranges - level2\n",
    "is_months_level2 = dates_train.isin(months_level2)\n",
    "dates_train_level2 = dates_train[is_months_level2]\n",
    "\n",
    "# That is how we get target for the 2nd level dataset\n",
    "y_train_level2 = y_train[is_months_level2]\n",
    "\n",
    "# And here we create 2nd level feature matrix, init it with zeros first\n",
    "X_train_level2 = np.zeros([y_train_level2.shape[0], 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "for cur_block_num in months_level2:\n",
    "\n",
    "    # track progress\n",
    "    print(cur_block_num)\n",
    "    \n",
    "    # idx range\n",
    "    is_train_K = dates_train < cur_block_num\n",
    "    is_test_K = dates_train == cur_block_num\n",
    "\n",
    "    # training subset\n",
    "    X_train_K = X_train[is_train_K]\n",
    "    y_train_K = y_train[is_train_K]\n",
    "    X_test_K = X_train[is_test_K]\n",
    "\n",
    "    # linear regression\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train_K.values, y_train_K)\n",
    "    pred_lr = lr.predict(X_test_K.values)\n",
    "\n",
    "    # LightGBM\n",
    "    model = lgb.train(lgb_params, lgb.Dataset(X_train_K, label=y_train_K), 100)\n",
    "    pred_lgb = model.predict(X_test_K)\n",
    "\n",
    "    # add OOF predictions\n",
    "    is_test_K_preds = dates_train_level2 == cur_block_num\n",
    "    X_train_level2[is_test_K_preds, 0] = pred_lr\n",
    "    X_train_level2[is_test_K_preds, 1] = pred_lgb   \n",
    "    \n",
    "# Sanity check\n",
    "assert np.all(np.isclose(X_train_level2.mean(axis=0), [ 1.50148988,  1.38811989]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the ensembles work best, when first level models are diverse. We can qualitatively analyze the diversity by examinig *scatter plot* between the two metafeatures. Plot the scatter plot below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f346f473588>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCnflgAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f346b6265f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "plt.scatter(X_train_level2[:, 0], X_train_level2[:, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when the meta-features are created, we can ensemble our first level models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple convex mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with simple linear convex mix:\n",
    "\n",
    "$$\n",
    "mix= \\alpha\\cdot\\text{linreg_prediction}+(1-\\alpha)\\cdot\\text{lgb_prediction}\n",
    "$$\n",
    "\n",
    "We need to find an optimal $\\alpha$. And it is very easy, as it is feasible to do grid search. Next, find the optimal $\\alpha$ out of `alphas_to_try` array. Remember, that you need to use train meta-features (not test) when searching for $\\alpha$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3468483390>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWuyPRTBTmj4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f34685dd5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alphas_to_try = np.linspace(0, 1, 1001)\n",
    "r2_scores = []\n",
    "\n",
    "# YOUR CODE GOES HERE\n",
    "for alpha in alphas_to_try:\n",
    "    mix_preds = alpha * X_train_level2[:, 0] + (1-alpha) * X_train_level2[:, 1]\n",
    "    r2_scores.append(r2_score(y_train_level2, mix_preds))\n",
    "\n",
    "plt.plot(alphas_to_try, r2_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.765000; Corresponding r2 score on train: 0.627255\n"
     ]
    }
   ],
   "source": [
    "best_alpha = np.array(r2_scores).argmax()/1000\n",
    "r2_train_simple_mix = max(r2_scores)\n",
    "\n",
    "print('Best alpha: %f; Corresponding r2 score on train: %f' % (best_alpha, r2_train_simple_mix))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the $\\alpha$ you've found to compute predictions for the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R-squared for simple mix is 0.781144\n"
     ]
    }
   ],
   "source": [
    "test_preds = best_alpha * X_test_level2[:, 0] + (1-best_alpha) * X_test_level2[:, 1]\n",
    "r2_test_simple_mix = r2_score(y_test, test_preds)\n",
    "\n",
    "print('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try a more advanced ensembling technique. Fit a linear regression model to the meta-features. Use the same parameters as in the model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "# linear regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_level2, y_train_level2)\n",
    "pred_train_level2 = lr.predict(X_train_level2)\n",
    "pred_test_level2 = lr.predict(X_test_level2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute R-squared on the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R-squared for stacking is 0.632176\n",
      "Test  R-squared for stacking is 0.771297\n"
     ]
    }
   ],
   "source": [
    "# train preds\n",
    "train_preds = lr.predict(X_train_level2)\n",
    "r2_train_stacking = r2_score(y_train_level2, train_preds)\n",
    "\n",
    "# test preds\n",
    "test_preds = lr.predict(X_test_level2)\n",
    "r2_test_stacking = r2_score(y_test, test_preds)\n",
    "\n",
    "# comparative performance\n",
    "print('Train R-squared for stacking is %f' % r2_train_stacking)\n",
    "print('Test  R-squared for stacking is %f' % r2_test_stacking)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, that the score turned out to be lower than in previous method. Although the model is very simple (just 3 parameters) and, in fact, mixes predictions linearly, it looks like it managed to overfit. **Examine and compare** train and test scores for the two methods. \n",
    "\n",
    "And of course this particular case does not mean simple mix is always better than stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all done! Submit everything we need to the grader now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task best_alpha is: 0.765\n",
      "Current answer for task r2_train_simple_mix is: 0.627255043446\n",
      "Current answer for task r2_test_simple_mix is: 0.781144169579\n",
      "Current answer for task r2_train_stacking is: 0.632175561459\n",
      "Current answer for task r2_test_stacking is: 0.771297132342\n"
     ]
    }
   ],
   "source": [
    "from grader import Grader\n",
    "grader = Grader()\n",
    "\n",
    "grader.submit_tag('best_alpha', best_alpha)\n",
    "\n",
    "grader.submit_tag('r2_train_simple_mix', r2_train_simple_mix)\n",
    "grader.submit_tag('r2_test_simple_mix',  r2_test_simple_mix)\n",
    "\n",
    "grader.submit_tag('r2_train_stacking', r2_train_stacking)\n",
    "grader.submit_tag('r2_test_stacking',  r2_test_stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STUDENT_EMAIL = 'plat.sebastien@hotmail.fr'\n",
    "STUDENT_TOKEN = '03lAtXodSDwvmrUo'\n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
