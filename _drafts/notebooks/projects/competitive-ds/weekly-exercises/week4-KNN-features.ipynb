{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 1.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will need to implement features, based on nearest neighbours. \n",
    "\n",
    "KNN classifier (regressor) is a very powerful model, when the features are homogeneous and it is a very common practice to use KNN as first level model. In this homework we will extend KNN model and compute more features, based on nearest neighbors and their distances. \n",
    "\n",
    "You will need to implement a number of features, that were one of the key features, that leaded the instructors to prizes in [Otto](https://www.kaggle.com/c/otto-group-product-classification-challenge) and [Springleaf](https://www.kaggle.com/c/springleaf-marketing-response) competitions. Of course, the list of features you will need to implement can be extended, in fact in competitions the list was at least 3 times larger. So when solving a real competition do not hesitate to make up your own features.   \n",
    "\n",
    "You can optionally implement multicore feature computation. Nearest neighbours are hard to compute so it is preferable to have a parallel version of the algorithm. In fact, it is really a cool skill to know how to use `multiprocessing`, `joblib` and etc. In this homework you will have a chance to see the benefits of parallel algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check your versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions we use here are not present in old versions of the libraries, so make sure you have up-to-date software. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.13.1\n",
      "pandas 0.20.3\n",
      "sklearn 0.19.0\n",
      "scipy 0.19.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn\n",
    "import scipy.sparse \n",
    "\n",
    "for p in [np, pd, sklearn, scipy]:\n",
    "    print (p.__name__, p.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The versions should be not less than:\n",
    "\n",
    "    numpy 1.13.1\n",
    "    pandas 0.20.3\n",
    "    sklearn 0.19.0\n",
    "    scipy 0.19.1\n",
    "   \n",
    "**IMPORTANT!** The results with `scipy=1.0.0` will be different! Make sure you use _exactly_ version `0.19.1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn features and labels. These features are actually OOF predictions of linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = '../readonly/KNN_features_data/X.npz'\n",
    "train_labels = '../readonly/KNN_features_data/Y.npy'\n",
    "\n",
    "test_path = '../readonly/KNN_features_data/X_test.npz'\n",
    "test_labels = '../readonly/KNN_features_data/Y_test.npy'\n",
    "\n",
    "# Train data\n",
    "X = scipy.sparse.load_npz(train_path)\n",
    "Y = np.load(train_labels)\n",
    "\n",
    "# Test data\n",
    "X_test = scipy.sparse.load_npz(test_path)\n",
    "Y_test = np.load(test_labels)\n",
    "\n",
    "# Out-of-fold features we loaded above were generated with n_splits=4 and skf seed 123\n",
    "# So it is better to use seed 123 for generating KNN features as well \n",
    "skf_seed = 123\n",
    "n_splits = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you need to implement features, based on nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NearestNeighborsFeats(BaseEstimator, ClassifierMixin):\n",
    "    '''\n",
    "    This class implements KNN features extraction.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_jobs, k_list, metric, n_classes=None, n_neighbors=None, eps=1e-6):\n",
    "        '''\n",
    "        + metric: metric used to train the KNN algorithm.\n",
    "        + k_list: values of K to apply the features generation to.\n",
    "        + n_jobs: number of threads for parallel processing when\n",
    "                  extracting features of datapoints.\n",
    "        + n_classes: number of classes to consider - only useful when\n",
    "                     some classes are not in the dataset. this number\n",
    "                     must be higher than the numer of classes in the \n",
    "                     dataset.\n",
    "        + n_neighbors: number of neighbors to use when fitting the KNN algorithm.\n",
    "                       if not provided, the algorithm will use max(k_list).\n",
    "        + eps: ensures we don't divide by zero.\n",
    "        '''\n",
    "        \n",
    "        # parameters for fitting the KNN algorithm\n",
    "        self.metric = metric\n",
    "        self.n_neighbors = max(k_list) if n_neighbors is None else n_neighbors\n",
    "            \n",
    "        # parameters for extracting features        \n",
    "        self.k_list = k_list\n",
    "        self.n_jobs = n_jobs\n",
    "        self.n_classes_ = n_classes\n",
    "        self.eps = eps\n",
    "       \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Setup the train set and self.NN object\n",
    "        '''\n",
    "        \n",
    "        # training labels\n",
    "        self.y_train = y\n",
    "        \n",
    "        # classes\n",
    "        self.n_classes = np.unique(y).shape[0] if self.n_classes_ is None else self.n_classes_\n",
    "        \n",
    "        \n",
    "        # Fit a NearestNeighbors (NN) object to X \n",
    "        self.NN = NearestNeighbors(n_neighbors=self.n_neighbors, \n",
    "                                   metric=self.metric, \n",
    "                                   n_jobs=1, \n",
    "                                   algorithm='brute' if self.metric=='cosine' else 'auto')\n",
    "        self.NN.fit(X)\n",
    "                \n",
    "            \n",
    "        \n",
    "    def predict(self, X):       \n",
    "        '''\n",
    "        Produces KNN features for every object of a dataset X\n",
    "        '''\n",
    "        \n",
    "        if self.n_jobs == 1:\n",
    "            test_feats = []\n",
    "            for i in range(X.shape[0]):\n",
    "                test_feats.append(self.get_features_for_one(X[i:i+1]))\n",
    "        \n",
    "        else:\n",
    "            '''\n",
    "            Number of threads is controlled by `self.n_jobs`.\n",
    "            Either use `multiprocessing.Pool` or `joblib`                     \n",
    "            '''\n",
    "            \n",
    "            gen = (X[i:i+1] for i in range(X.shape[0]))\n",
    "            pool = Pool(processes=self.n_jobs)\n",
    "            test_feats = pool.map(self.get_features_for_one, gen)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            \n",
    "            # Comment out this line once you implement the code\n",
    "            # assert False, 'You need to implement it for n_jobs > 1'\n",
    "            \n",
    "        return np.vstack(test_feats)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_features_for_one(self, x):\n",
    "        '''\n",
    "        Computes KNN features for a single object `x`\n",
    "        '''\n",
    "\n",
    "        NN_output = self.NN.kneighbors(x)\n",
    "        \n",
    "        # vectors of size `n_neighbors`\n",
    "        neighs = NN_output[1][0]        # neighbors indices        \n",
    "        neighs_dist = NN_output[0][0]   # distances to neighbors\n",
    "        neighs_y = self.y_train[neighs] # labels of neighbors\n",
    "        \n",
    "        # list of computed features (each feature is a list or np.array)\n",
    "        features_list = [] \n",
    "        \n",
    "        # add features\n",
    "        \n",
    "        \n",
    "        features_list += self.neighbors_class_probabilities_(neighs_dist, neighs_y)\n",
    "        features_list += self.first_neighbors_with_same_class_(neighs_y)\n",
    "        features_list += self.minimum_distance_to_class_(neighs_dist, neighs_y)\n",
    "        features_list += self.minimum_normed_distance_to_class_(neighs_dist, neighs_y)\n",
    "        features_list += self.distance_to_kth_neighbor_(neighs_dist, neighs_y)\n",
    "        features_list += self.mean_distance_to_class_(neighs_dist, neighs_y)\n",
    "        \n",
    "        # merge features\n",
    "        knn_feats = np.hstack(features_list)\n",
    "        \n",
    "        assert knn_feats.shape == (239,) or knn_feats.shape == (239, 1)\n",
    "        return knn_feats\n",
    "    \n",
    "    \n",
    "    \n",
    "    def neighbors_class_probabilities_(self, neighs_dist, neighs_y):\n",
    "        ''' \n",
    "        Fraction of neighbors for every class (KNNĞ¡lassifiers predictions).\n",
    "        Returns a list of length `len(k_list) * n_classes`.\n",
    "        '''\n",
    "        \n",
    "        # use of `np.bincount`\n",
    "        feature = []       \n",
    "        for k in self.k_list:\n",
    "            neighs_y_k = neighs_y[:k]\n",
    "            feature_k = np.bincount(neighs_y_k, minlength=self.n_classes) / neighs_y_k.shape[0]\n",
    "            feature += feature_k.tolist()\n",
    "        \n",
    "        return feature\n",
    "        \n",
    "        \n",
    "        \n",
    "    def first_neighbors_with_same_class_(self, neighs_y):\n",
    "        '''\n",
    "        K first neighbors with the same label.\n",
    "        Returns a list of size `1` with a single integer.\n",
    "        '''\n",
    "\n",
    "        # use of `np.where`\n",
    "        non_matching_idx = np.where(neighs_y!=neighs_y[0])[0]\n",
    "        if non_matching_idx.size == 0:\n",
    "            feature = self.n_neighbors\n",
    "        else:\n",
    "            feature = non_matching_idx[0]\n",
    "\n",
    "        return [feature]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def minimum_distance_to_class_(self, neighs_dist, neighs_y):  \n",
    "        '''\n",
    "        Minimum distance to neighbors of each class; 999 if no neighbors of a class.\n",
    "        Returns a list of length `n_classes`.\n",
    "        '''\n",
    "                \n",
    "        # use of `np.where`\n",
    "        feature = [999] * self.n_classes\n",
    "        for c in range(self.n_classes):\n",
    "            c_dist = np.where(neighs_y==c, neighs_dist, 999)\n",
    "            feature[c] = np.min(c_dist)\n",
    "            \n",
    "        return feature\n",
    "        \n",
    "        \n",
    "        \n",
    "    def minimum_normed_distance_to_class_(self, neighs_dist, neighs_y):\n",
    "        '''\n",
    "        Minimum distance to neighbors of each class, \"normalized\"\n",
    "        by the distance to closest class; 999 if no neighbors of a class.\n",
    "        Returns a list of length `n_classes`.\n",
    "        '''\n",
    "           \n",
    "        # use of self.eps\n",
    "        min_distances = self.minimum_distance_to_class_(neighs_dist, neighs_y)\n",
    "        min_dist = min(min_distances) + self.eps\n",
    "        \n",
    "        feature = [dist/min_dist if dist < 999 else 999 for dist in min_distances]\n",
    "        return feature\n",
    "       \n",
    "        \n",
    "    \n",
    "    def distance_to_kth_neighbor_(self, neighs_dist, neighs_y):\n",
    "        '''\n",
    "        Distance to each neighbor & distance normalized\n",
    "        by the distance to closest neighbor.\n",
    "        Returns a list of dimensions `1x2*len(k_list)`.\n",
    "        '''\n",
    "        \n",
    "        # use of self.eps\n",
    "        feature = []\n",
    "        for k in self.k_list:\n",
    "            \n",
    "            neighs_dist_k = neighs_dist[:k]\n",
    "            min_dist = min(neighs_dist_k)\n",
    "            feat_51 = neighs_dist_k[k-1]\n",
    "            feat_52 = neighs_dist_k[k-1] / (min_dist + self.eps)\n",
    "            feature += [[feat_51, feat_52]]\n",
    "            \n",
    "        return feature\n",
    "            \n",
    "    \n",
    "    \n",
    "    def mean_distance_to_class_(self, neighs_dist, neighs_y):\n",
    "        '''\n",
    "        Mean distance to neighbors of each class; 999 if no neighbors of a class.\n",
    "        Returns a list of length `len(k_list) * n_classes`.\n",
    "        '''\n",
    "        \n",
    "        feature = []\n",
    "        for k in self.k_list:\n",
    "            \n",
    "            # info for the k-NN\n",
    "            neighs_y_k = neighs_y[:k]\n",
    "            neighs_dist_k = neighs_dist[:k]\n",
    "            \n",
    "            # extract sum of distances & count of labels for each class\n",
    "            dist_per_class = np.bincount(neighs_y_k, weights= neighs_dist_k, minlength=self.n_classes)\n",
    "            y_per_class = np.bincount(neighs_y_k, minlength=self.n_classes)\n",
    "            \n",
    "            # get avg distance\n",
    "            feature_k = np.where(y_per_class==0, 999, dist_per_class / (y_per_class + self.eps))            \n",
    "            feature += feature_k.tolist()\n",
    "            \n",
    "        return feature\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure you've implemented everything correctly we provide you the correct features for the first 50 objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deviation from ground thruth features: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# a list of K in KNN, starts with one \n",
    "k_list = [3, 8, 32]\n",
    "\n",
    "# Load correct features\n",
    "true_knn_feats_first50 = np.load('../readonly/KNN_features_data/knn_feats_test_first50.npy')\n",
    "\n",
    "# Create instance of our KNN feature extractor\n",
    "NNF = NearestNeighborsFeats(n_jobs=4, k_list=k_list, metric='minkowski')\n",
    "\n",
    "# Fit on train set\n",
    "NNF.fit(X, Y)\n",
    "\n",
    "# Get features for test\n",
    "test_knn_feats = NNF.predict(X_test[:50])\n",
    "\n",
    "# This should be zero\n",
    "print ('Deviation from ground thruth features: %f' % np.abs(test_knn_feats - true_knn_feats_first50).sum())\n",
    "\n",
    "deviation =np.abs(test_knn_feats - true_knn_feats_first50).sum(0)\n",
    "for m in np.where(deviation > 1e-3)[0]: \n",
    "    p = np.where(np.array([87, 88, 117, 146, 152, 239]) > m)[0][0]\n",
    "    print ('There is a problem in feature %d, which is a part of section %d.' % (m, p + 1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement parallel computations and compute features for the train and test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get features for test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute features for the whole test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minkowski\n",
      "cosine\n"
     ]
    }
   ],
   "source": [
    "for metric in ['minkowski', 'cosine']:\n",
    "    print (metric)\n",
    "    \n",
    "    # Create instance of our KNN feature extractor\n",
    "    NNF = NearestNeighborsFeats(n_jobs=4, k_list=k_list, metric=metric)\n",
    "    \n",
    "    # Fit on train set\n",
    "    NNF.fit(X, Y)\n",
    "\n",
    "    # Get features for test\n",
    "    test_knn_feats = NNF.predict(X_test)\n",
    "    \n",
    "    # Dump the features to disk\n",
    "    np.save('knn_feats_%s_test.npy' % metric , test_knn_feats)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get features for train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute features for train, using out-of-fold strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minkowski\n",
      "cosine\n"
     ]
    }
   ],
   "source": [
    "#16:10\n",
    "\n",
    "# Differently from other homework we will not implement OOF predictions ourselves\n",
    "# but use sklearn's `cross_val_predict`\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# We will use two metrics for KNN\n",
    "for metric in ['minkowski', 'cosine']:\n",
    "    print (metric)\n",
    "    \n",
    "    # Set up splitting scheme, use StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits, shuffle=True, random_state=skf_seed)\n",
    "    \n",
    "    # Create instance of our KNN feature extractor\n",
    "    NNF = NearestNeighborsFeats(n_jobs=4, k_list=k_list, metric=metric)\n",
    "    \n",
    "    # Get KNN features using OOF use cross_val_predict with right parameters\n",
    "    preds = cross_val_predict(NNF, X, Y, cv=skf)\n",
    "    \n",
    "    # Save the features\n",
    "    np.save('knn_feats_%s_train.npy' % metric, preds)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you made the above cells work, just run the following cell to produce a number to submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3838.0\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for metric in ['minkowski', 'cosine']:\n",
    "    knn_feats_train = np.load('knn_feats_%s_train.npy' % metric)\n",
    "    knn_feats_test = np.load('knn_feats_%s_test.npy' % metric)\n",
    "    \n",
    "    s += knn_feats_train.mean() + knn_feats_test.mean()\n",
    "    \n",
    "answer = np.floor(s)\n",
    "print (answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task statistic is: 3838.0\n",
      "You want to submit these numbers:\n",
      "Task statistic: 3838.0\n",
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "from grader import Grader\n",
    "\n",
    "grader = Grader()\n",
    "\n",
    "grader.submit_tag('statistic', answer)\n",
    "\n",
    "STUDENT_EMAIL = 'plat.sebastien@hotmail.fr'\n",
    "STUDENT_TOKEN = 'r3JKJUIMgA202mjV'\n",
    "grader.status()\n",
    "\n",
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
