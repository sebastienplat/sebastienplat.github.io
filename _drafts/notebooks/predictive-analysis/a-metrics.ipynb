{
 "cells": [
  {
   "source": [
    "# TOC\n",
    "\n",
    "+ Machine Learning Definitions\n",
    "+ Performance Metrics - Regression\n",
    "+ Performance Metrics - Classification\n",
    "+ Loss vs Metrics\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Machine Learning\n",
    "## I.1. Regression vs Classification\n",
    "\n",
    "TODO.\n",
    "\n",
    "Performance metrics are used to assess the quality of an algorithm; you must choose the most appropriate one for the task at hand. Let's take the example of an online shop that tries to maximize the effectiveness of their website. The company must decide the measure they want to use to quantify effectiveness, which is the measure they will try to optimize. It can be the number of visits, the ratio of visits that led to orders, etc.\n"
   ]
  },
  {
   "source": [
    "## I.2. Other ML Problems\n",
    "\n",
    "### Ranking\n",
    "\n",
    "+ [Overview of the topic](https://wellecks.wordpress.com/2015/01/15/learning-to-rank-overview/)\n",
    "+ [RankNet introduction](https://icml.cc/2015/wp-content/uploads/2015/06/icml_ranking.pdf) - pairwise method for AUC optimization\n",
    "+ [RankNet improvements](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf)\n",
    "+ [Library of LTR algorithms](https://sourceforge.net/p/lemur/wiki/RankLib/)\n",
    "\n",
    "\n",
    "### Clustering\n",
    "\n",
    "+ [Evaluation metrics for clustering](http://nlp.uned.es/docs/amigo2007a.pdf)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# II. Performance Metrics - Regression\n",
    "## II.1. Mean Squared Error, Root Mean Squared Error and R²\n",
    "\n",
    "$MSE = \\frac{1}{N} \\sum\\limits_{i=1}^N (y_i - \\hat y _i)^2$\n",
    "\n",
    "$RSME = \\sqrt{MSE}$\n",
    "\n",
    "MSE vs RMSE:\n",
    "+ the RMSE has the same unit as the values to predict, so it's easier to compare.\n",
    "+ any minimizer of RMSE is a minimizer of RMSE and vice-versa: $MSE(a) > MSE(b) <=> RMSE(a) > RMSE(b)$. This means that optimizing for MSE is the same as optimizing for RMSE.\n",
    "\n",
    "_Note: the MSE is easier to use than RMSE for gradient descent: the RMSE gradient is a function of the MSE, which means its learning rate is dynamic._\n",
    "\n",
    "_Note: the MSE can be linked to the L2-loss. See this [SO question](https://datascience.stackexchange.com/questions/26180/l2-loss-vs-mean-squared-loss) for more details._\n",
    "\n",
    "**MSE Baseline (optimal constant)**\n",
    "\n",
    "When predicting a constant value, the MSE and RMSE are the smallest for $\\bar y = \\frac{1}{N} \\sum\\limits_{i=1}^N y_i$. In this case, the MSE is equal to the variance.\n",
    "\n",
    "\n",
    "**R squared**\n",
    "\n",
    "The value of the MSE itself doesn't tell us how good the model really is: it only allows us to compare the performance of various models. R Squared can be used for that purpose: it compares a model performance with the performance of the constant value that has the smallest MSE (predicting a constant value is the most naive approach and is used as the baseline performance). \n",
    "\n",
    "R² takes values from 0 (when the model performs no better than using the constant $\\bar y$ for all predictions) to 1 (when the model has perfect predictions):\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\frac{1}{N} \\sum\\limits_{i=1}^N (y_i - \\hat y _i)^2}{\\frac{1}{N} \\sum\\limits_{i=1}^N (y_i - \\bar y _i)^2} = 1 - \\frac{MSE}{Var(y)}$$\n",
    "\n",
    "_Note: the formula of R² shows that optimizing for R² is the same as optimizing for MSE._\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## II.2. Mean Absolute Error\n",
    "\n",
    "$MAE = \\frac{1}{N} \\sum\\limits_{i=1}^N |y_i - \\hat y _i|$\n",
    "\n",
    "The MAE is close to the MSE but does not penalize large errors as badly, so it's less sensitive to outliers. It's better suited if the impact of large errors is proportionally the same as small errors (i.e. if an error of 10 costs twice as much as an error of 5, not more). It is widely used in finance.\n",
    "\n",
    "_Note: the MAE is a specific type of quantile loss._\n",
    "\n",
    "_Note: the MAE can be linked to the L1-loss._\n",
    "\n",
    "_Note: the absolute function is not differentiable at x = 0, so the MAE has no gradient if the predictions perfectly match the values to predict._\n",
    "\n",
    "**MAE Baseline (optimal constant)**\n",
    "\n",
    "When predicting a constant value, the MAE is the smallest for the median of $y$.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## II.3. MSE vs MAE\n",
    "\n",
    "MAE is robust to outliers, which means they will not influence predictions significantly. If outliers are mistakes, then MAE will be a better choice. Otherwise, MSE will be more suited to the task.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## II.4. MSPE & MAPE\n",
    "\n",
    "MSE and MAE work with absolute errors: predicting 10 for a correct value of 9 and predicting 1000 for a correct value of 1 lead to the same SE and AE of 1. We can use the Mean Squared Percentage Error and Mean Absolute Percentage Error instead:\n",
    "\n",
    "$$MSPE = \\frac{100\\%}{N} \\sum\\limits_{i=1}^N \\left( \\frac{y_i - \\hat y _i}{y_i} \\right)^2$$\n",
    "\n",
    "$$MAPE = \\frac{100\\%}{N} \\sum\\limits_{i=1}^N \\left\\lvert \\frac{y_i - \\hat y _i}{y_i} \\right\\rvert$$\n",
    "\n",
    "The cost for a fixed absolute error decreases as values get larger: MSPE and MAPE are weighted versions of MSE and MAE. Their optimal constant are the weighted mean and weighted median, respectively, with weights equal to:\n",
    "    \n",
    "$$\\mathrm{MSPE}: w_i = \\frac{1/y{_i}{^2}} {\\sum\\limits_{j=1}^N 1/y{_j}{^2}}$$\n",
    "\n",
    "$$\\mathrm{MAPE}: w_i = \\frac{1/y{_i}} {\\sum\\limits_{j=1}^N 1/y{_j}}$$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## II.5. RMSLE\n",
    "\n",
    "The Root Mean Squared Logarithmic Error is the RMSE calculated at the logarithmic scale:\n",
    "\n",
    "$$RMSLE = \\sqrt{\\frac{1}{N} \\sum\\limits_{i=1}^N ( ln(y_i + 1) - ln(\\hat y _i + 1))^2}$$\n",
    "\n",
    "It is similar to the MSPE in the sense that it penalizes errors of small values more, but it penalizes underpredictions more than overpredictions. Its optimal constant in the log space is the weighted mean, and we need to exponentiate to get the actual value.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# III. Performance Metrics - Classification\n",
    "\n",
    "_Note: a cheat sheet of metrics used for classification can be found [here](https://queirozf.com/entries/evaluation-metrics-for-classification-quick-examples-references)._\n",
    "\n",
    "## III.1. Definitions\n",
    "\n",
    "The terms \"positive\" and \"negative\" do not refer to benefit, but to the presence or absence of a condition: sick vs healthy, correctly classified vs not, etc.\n",
    "\n",
    "\n",
    "|     TOTAL         | \\| |           POS              |              NEG                  | \\| |   prevalence = POS / TOTAL       |\n",
    "|:-----------------:|:--:|:--------------------------:|:---------------------------------:|:--:|:--------------------------------:|\n",
    "| ----------------- | \\| | -------------------------- | --------------------------------- | \\| | -------------------------------- |\n",
    "| **PP (Pred Pos)** | \\| |        **TP**              |        **FP** (error I)           | \\| |  **precision** = TP / PP         |\n",
    "| **PN (Pred Neg)** | \\| |   **FN** (error II)        |             **TN**                | \\| |                                  |\n",
    "| ----------------- | \\| | -------------------------- | --------------------------------- | \\| | -------------------------------- |\n",
    "|        .          | \\| |  **recall** = TP / POS     | *false alarm = 1 - specificity*   | \\| | **accuracy** = (TP + TN) / TOTAL |\n",
    "|        .          | \\| | *miss rate = 1 - recall*   |     **specificity** = TN / NEG    | \\| |      **F-score** (see below)     |\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Note: many terms exist to describe one measure.\n",
    "\n",
    "|     TOTAL         | \\| |           POS                           |              NEG                    | \\| |   prevalence                              |    |\n",
    "|:-----------------:|:--:|:---------------------------------------:|:-----------------------------------:|:--:|:-----------------------------------------:|:--:|\n",
    "| ----------------- | \\| | --------------------------------------- | ----------------------------------- | \\| | ----------------------------------------- | --------------------\n",
    "| **PP (Pred Pos)** | \\| |        **TP**                           |        **FP** (error I)             | \\| | **precision** / positive predictive value | False Discovery Rate\n",
    "| **PN (Pred Neg)** | \\| |   **FN** (error II)                     |             **TN**                  | \\| | False Omission Rate                       | NPV\n",
    "| ----------------- | \\| | --------------------------------------- | ----------------------------------  | \\| | ----------------------------------------- | --------------------\n",
    "|        .          | \\| |  **recall** / sensitivity / power / TPR | *false alarm / fall-out / FPR*      | \\| | **accuracy**                              |\n",
    "|        .          | \\| | *miss rate / FNR*                       | **specificity** / selectivity / TNR | \\| | **F-score**                               |"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Sensitivity vs Specificty\n",
    "\n",
    "These two terms are widely used in medicine:\n",
    "\n",
    "+ sensitivity measures the proportion of positives that are correctly identified. The smaller the error II, the higher the recall.\n",
    "+ specificiy measures the proportion of negatives that are correctly identified. The smaller the error I, the higher the specificity.\n",
    "\n",
    "They are prevalence-independent test characteristics, as their values are intrinsic to the test and do not depend on the disease prevalence in the population of interest.\n",
    "\n",
    "\n",
    "### Recall vs Precision\n",
    "\n",
    "These two terms are widely used in information retrieval:\n",
    "\n",
    "+ recall is the probability that a relevant document is retrieved by the query.\n",
    "+ precision is the probability that a retrieved documents is relevant to the query.\n",
    "\n",
    "\n",
    "### F-Score\n",
    "\n",
    "The $F_{\\beta}$-score combines recall & precision to mesure the accuracy of a given test. The value of $\\beta$ adds weight to the recall to adjust its comparative importance to the resulting score. In other terms, $\\beta$ adds more weights either to false negatives or false positives.\n",
    "\n",
    "\n",
    "$$F_{\\beta} = (1 + \\beta^2) \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{(\\beta^2 \\cdot \\mathrm{precision}) + \\mathrm{recall}} = \\frac {(1 + \\beta^2) \\cdot \\mathrm{true\\ positive} }{(1 + \\beta^2) \\cdot \\mathrm{true\\ positive} + \\beta^2 \\cdot \\mathrm{false\\ negative} + \\mathrm{false\\ positive}}$$\n",
    "\n",
    "Commonly used values of $\\beta$ are 0.5, 1 and 2. Note that $F_1$ is the harmonic mean of precision & recall:\n",
    "\n",
    "$$F_1 = \\frac{2}{\\mathrm{recall}^{-1} + \\mathrm{precision}^{-1}}$$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Soft Predictions vs Hard-Labels\n",
    "\n",
    "+ soft predictions are the classifier's score for a given observation:\n",
    "    + for multiclass problems, it's a vector of size L where L is the number of classes. Each value is the probability that the observation belongs to a given class. The sum of probabilities in the vector are exactly one.\n",
    "    + for binary classification, it is a single value: the probability that the observation belongs to the positive class.\n",
    "+ hard labels are a function of soft predictions: it's the $\\mathrm{argmax}_i(probs)$ of the scores vector for multi-classes and a threshold $prob > b$ for binary classification. The default threshold is 0.5 but can be tweaked to get better results - see AUC below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## III.2. Accuracy vs Balanced Accuracy\n",
    "\n",
    "The accuracy is the probability that a value is correctly classified; it uses hard labels. It can be a misleading metric for imbalanced data sets: for a sample with 95 negative and 5 positive values, predicting all values as negative yields an accuracy of (0 + 95) / 100 = 95%.\n",
    "\n",
    "The balanced accuracy is the mean of the TPR and TNR, i.e. the average true predictions across classes. In our example, the balanced accuracy is 0.5 x (95/95 + 0/5) = 0.5.\n",
    "\n",
    "_Note: the best constant value is to predict the most frequent class._\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## III.3. Cross-Entropy / Log-Loss\n",
    "\n",
    "The logloss greatly penalizes completely wrong scores: it prefers to make many small mistakes rather than a few large ones. A detailed explaination of the binary cross-entropy / logloss can be found [here](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a).\n",
    "\n",
    "_Note: the best constant value is a vector that lists the frequencies of each class in the dataset._\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## III.4. AUC ROC\n",
    "\n",
    "The [ROC Curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) plots the recall as a function of the fall-out, i.e. TPR = f(FPR), when applying various discrimination thresholds (values above which a soft score is considered to belong to the positive class). It illustrates the trade-off between sensitivity and specificity, i.e. between error I vs error II. Because it takes into account all possible thresholds, it removes the effect of having to carefully choose a threshold.\n",
    "\n",
    "The Area Under the ROC Curve gives the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one (assuming 'positive' ranks higher than 'negative'). If we consider all possible \\[negative, positive\\] pairs and compare their soft scores, the AUC is the fraction of pairs where the positive score is higher than the negative score.\n",
    "\n",
    "The AUC is a [visual representation](http://www.navan.name/roc/) of how much the probability distributions of the positive and negative classes overlap for our model.\n",
    "\n",
    "_Note: AUC only measures how well predictions are ranked, rather than their absolute values. It means that it is scale-invariant (i.e. multiplying all predictions by the same constant). It follows that all constant values will lead to the same AUC._\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## III.5. Cohen's Kappa\n",
    "\n",
    "Coehn's Kappa fixes the accuracy of a random guess to be zero, and compares each model to this baseline; it's similar to how R squared works for regression. See the [wikipedia article](https://en.wikipedia.org/wiki/Cohen%27s_kappa) and this [paper](https://arxiv.org/abs/1509.07107) for more details.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "_Note: a cheat sheet of metrics used for classification can be found [here](https://queirozf.com/entries/evaluation-metrics-for-classification-quick-examples-references)._"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# IV. Loss vs Metrics\n",
    "\n",
    "+ the metric is what we want to optimize: what we'll use to measure the quality of our model.\n",
    "+ the optimization loss function is what the model will optimize.\n",
    "\n",
    "Sometimes the metric can be used as a loss function (MSE, logloss, etc.), but it's not always possible. It might be possible to preprocess the data & optimize another metric that results in directly optimizing the metric of interest  (MSPE, MAPE). Another option would be to optimize another metric and postprocess the predictions (accuracy, Cohen's Kappa).\n",
    "\n",
    "Another solution that works in all circumstances is to use early stopping: train a model using any loss function and measure the metric of interest on a validation set. The training stops when the metric starts to overfit the training set (i.e. its performance increases in the training set but decreases in the validation set).\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## IV.1. Regression Metrics as Optimizers\n",
    "\n",
    "|        Metric        | XGBoost | LightGBM | SKL.RandomForestRegressor | SKL.<prefix>Regression | SKL.SGDRegressor | NN Libraries |\n",
    "|:--------------------:|:-------:|:--------:|:-------------------------:|:----------------------:|:----------------:|:------------:|\n",
    "| MSE / MSPE* / MSLE** |    x    |     x    |             x             |            x           |         x        |       x      |\n",
    "|      MAE / MAPE      |    -    |     x    |             x             |            -           |         -        |       x      |\n",
    "\n",
    "*For MSPE and MAPE, there are two options: either use the `sample_weights` parameter when the libray allows it or simply resample the train set: `df.sample(weights=sample_weights)`. Note that results will be better and more stable when resampling several times and averaging the predictions.\n",
    "\n",
    "**For MSLE, you can optimize MSE when training the model on the logarithmic scale using $z_i = ln(y_i + 1)$, then convert the predictions on the test set back to its original scale: $\\hat{y_i} = exp(\\hat{z_i}) - 1$.\n",
    "\n",
    "The [Huber loss](https://en.wikipedia.org/wiki/Huber_loss) combines the best properties of both L1 and L2 losses. Its values are equal to the L2 loss for small values centered around zero (which means its derivative exists for zero) and to the L1 loss otherwise, making it less sensitive to outliers.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## IV.3. Classification Metrics as optimizers\n",
    "\n",
    "|  Metric    | XGBoost | LightGBM | SKL.RandomForestRegressor | SKL.<prefix>Regression | SKL.SGDRegressor | NN Libraries |\n",
    "|:----------:|:-------:|:--------:|:-------------------------:|:----------------------:|:----------------:|:------------:|\n",
    "| Logloss    |    x    |     x    |             -             |            x           |         x        |       x      |\n",
    "| Accuracy   |    -    |     -    |             -             |            -           |         -        |       -      |\n",
    "| AUC        |    x    |     x    |             -             |            -           |         -        |       o      |\n",
    "\n",
    "If a model cannot directly optimize logloss, we can calibrate its results by fitting an another model to its predictions that will be optimized with logloss:\n",
    "\n",
    "+ Platt scaling: fit a Logistic Regression to the predictions.\n",
    "+ Isotonic scaling: fit an Isotonic Regression to the predictions.\n",
    "+ Stacking: fit XGBost or NN to the predictions.\n",
    "\n",
    "Accuracy cannot be optimized directly because its value is either one (the class is correctly predicted) or zero (the class is incorrectly predicted). It means that its gradient is zero most of the time so gradient descent cannot work. Training a model with any loss function and performing threshold-tuning is the best approach.\n",
    "\n",
    "The AUC is a measure of pairwise loss as discussed in the AUC section above. Its implementation for NN libraries can be found online.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 32-bit",
   "name": "python38032bit64a64ed7a47843b8be3706a54e9a0958"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}