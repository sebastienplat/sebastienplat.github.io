{
 "cells": [
  {
   "source": [
    "# TOC\n",
    "\n",
    "+ EDA\n",
    "+ Missing Values Imputation\n",
    "+ Features Preprocessing\n",
    "+ Features Generation\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. EDA\r\n",
    "\r\n",
    "This process helps understand the data, build intuition about it, generate hypotheses and find insights. It can also be used to check if train and test set have similar populations (men vs women, e)t It is also useful to compare the distribution of each feature between training and test set; if they are very different, we need to find ways to make it match or exclude the feature entirely.c.\n",
    "\n",
    "**individual features**\n",
    "+ numerical summary / statistics.\n",
    "+ histogram. \n",
    "+ plotting row index vs value.\n",
    "\n",
    "**pairs of features**\n",
    "+ scatter plots. A few interesting tweaks:\n",
    "    + add a color for each class (classification) or match point size with outcome value (regression).\n",
    "    + overlapping test set with train set to see if values match.\n",
    "+ scatter matrix.\n",
    "\n",
    "**groups of features**\n",
    "+ correlation matrix. Running [K-means clustering](https://scikit-learn.org/stable/auto_examples/bicluster/plot_spectral_biclustering.html) can help group related features together.\n",
    "+ plotting column index vs statistics value (like mean) of each feature\n",
    "**impact on target variable**\n",
    "+ scatterplot.\n",
    "+ binning.\n",
    ".\n",
    "\n",
    "It can be helpful to generate new features based on each of these group of feature\n",
    ")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df values\r\n",
    "df.describe()\r\n",
    "x.value_counts()\r\n",
    "x.isnull()\r\n",
    "\r\n",
    "# plot values vs index\r\n",
    "plt.plot(x, '.')\r\n",
    "plt.scatter(range(len(x)), x, c=y)\r\n",
    "\r\n",
    "# correlation (scatter plots, correlation matrix)\r\n",
    "pd.scatter_matrix(df)\r\n",
    "df.corr()\r\n",
    "plt.matshow(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Missing Values Imputation\r\n",
    "\r\n",
    "The scikit-learn documentation has a section dedicated to [missing values imputation](https://scikit-learn.org/stable/modules/impute.html\n",
    "\n",
    "Missing values are sometimes not loaded as NaN: they might have been replaced by a single value that is completely out of the range taken by the rest of the values. These cases can be found by plotting an histogram.\n",
    "\n",
    "Once identified, missing values can be inputed in a few ways:\n",
    "+ inferred. This method should be handled with caution, especially when using inferred values to generate a new feature.\n",
    "+ use a single value outside the feature's value range (-1, -999, etc.). This can be used as a separate category but will penalize non tree-based models.\n",
    "+ use the meanrof the median. This works well for non-tree based methods but tree-based models won't be able to easily create a split for missing values.\n",
    "\n",
    "An option is to add a new binary feature to flag rows that had missing values, then use either mean or median. The downside is that this method will double the number of features in the dataset.\n",
    "\n",
    "For categorical data, we can use frequency encoding to highlight categories that are in the test set but not the training s.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Features Preprocessing\r\n",
    "\r\n",
    "Features preprocessing & generation pipelines depend on the model type. A few examples:\r\n",
    "\r\n",
    "+ A categorical feature that happens to be stored as numerical will not perform well with a linear model if the relation with the outcome is linear. In this case, one-hot encoding will perform better. But this preprocessing step is not required to fit a random forest.\r\n",
    "+ Forecasting a linear trend will work well with a linear model, but a tree-based approach will not create splits for unseen dates and might perform poorly.\r\n",
    "\r\n",
    "The scikit-learn documentation has a section dedicated to [preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.1. Numeric Features\n",
    "\n",
    "Tree-based models are not impacted by feature scales nor outliers.\n",
    "\n",
    "**Feature Scale (non-tree based)**\n",
    "\n",
    "Non tree-based models (KNN, linear models & NN) are strongly impacted by the scale of each feature:\n",
    "+ KNN: predictions are based on distances, so they will vary significantly depending on the scale of each feature.\n",
    "+ Linear models & NN: \n",
    "    + regularization impact is proportional to feature scale. It will work best if we can apply it to each coefficient in equal amounts.\n",
    "    + gradient descent methods don't work well without features scaling.\n",
    "\n",
    "The easiest way to deal with this issue is to rescale all features to the same scale:\n",
    "+ `sklearn.preprocessing.MinMaxScaler` scale to \\[0, 1\\]: $X = (X - X.min()) / (X.max() - X.min())$. The distribution of values doesn't change.\n",
    "+ `sklearn.preprocessing.StandardScaler` scale to mean=0 and std=1: $X = (X - X.mean()) / X.std()$. \n",
    "\n",
    "_Note: Different scalings result in different model quality: it is another hyperparameter you need to optimize._\n",
    "\n",
    "_Note: when using KNN, we can optimize the scaling parameters for certain features in order to boost their impact on predictions._\n",
    "\n",
    "An analysis of when to use min-max vs standardization can be found [here](https://sebastianraschka.com/Articles/2014_about_feature_scaling.html).\n",
    "\n",
    "**Outliers [Winsorizing](https://en.wikipedia.org/wiki/Winsorizing) (linear models)**\n",
    "\n",
    "Outliers (for both features and target values) can impact linear models significantly. Clipping feature values between some lower and upper bounds (like 1st and 99th percentiles) can mitigate this issue. This method is frequently used with financial data and is called winsorization.\n",
    "\n",
    "\n",
    "**Rank Transformation (non-tree based)**\n",
    "\n",
    "Rank transformation sets the space between values to be equal. A quick way of handling outliers is to use the values indices instead of their values (see `scipy.stats.rankdata`). The transformation used in the training set needs to be applied to test and validation sets.\n",
    "\n",
    "**Log/Sqrt Transforms (non-tree based)**\n",
    "\n",
    "Applying `np.log(1+x)` or `np.sqrt(x + 2/3)` to a feature can benefit all non tree-based models, especially NN, as they:\n",
    "\n",
    "+ bring extreme values of the feature closer to the average.\n",
    "+ make values close to zero more easily distinguishable.\n",
    "\n",
    "**Grouping Preprocessings**\n",
    "\n",
    "Training a model on concatenated dataframes, each having gone through different preprocessings, can sometimes yield great results. Another option is to mix models trained on differently preprocessed data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.2. Categorical & Ordinal Data\n",
    "\n",
    "_Reminder: \"ordinal\" means \"ordered categorical\" (start ratings, level of education, etc.). We cannot be sure that the difference between values is always constant (the difference between four and five stars might be smaller than between two and three stars)._\n",
    "\n",
    "**Label Encoding (tree based)**\n",
    "\n",
    "Label encoding maps categories into numbers. This method works well with tree-based models: they can split the feature and extract the most useful values.\n",
    "\n",
    "+ `sklearn.preprocessing.LabelEncoder`: apply the encoding in sorted order.\n",
    "+ `pandas.factorize`: apply the encoding by order of appearance.\n",
    "\n",
    "**Frequency Encoding (tree based)**\n",
    "\n",
    "Frequency encoding (see below) uses the frequency of each value as key.\n",
    "\n",
    "_Note: if two categories have the same frequency, they won't be distinguishable after frequency encoding alone. In this case, a ranking operation will help._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency encoding\n",
    "encoding = df.groupby(feature).size() # number of occurrences by value\n",
    "encoding = encoding / len(df)         # frequency of each value\n",
    "df['enc_feature'] = df['feature'].map(encoding)\n"
   ]
  },
  {
   "source": [
    "**One-Hot Encoding**\n",
    "\n",
    "One-Hot Encoding creates a new boolean column for each value (it is by definition already min-max-scaled):\n",
    "+ `sklearn.preprocessing.OneHotEncoder`\n",
    "+ `pandas.get_dummies`\n",
    "\n",
    "_Note: tree-based methods will struggle to use numeric features efficiently if there are too many binary columns created via one-hot encoding (they will not be selected in enough random splits)._\n",
    "\n",
    "_Note: one-hot encoding features with a large amount of categories will create many binary features that have few non-zero values. In these cases, sparce matrices will have much better performance because they only store non-null values in memory._\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## III.3. Target Mean Encoding\n",
    "\n",
    "Mean encoding takes the mean of the target for each category of the feature in the test set. It ranks categories by target mean value and makes it easier for algorithms to use the feature. By contrast, label encoding orders categories at random, which makes it harder for algorithms to use the feature.\n",
    "\n",
    "More concretely, it allows tree-based algorithms to get the same level of performance with shorter trees, especially with high-cardinality categorical features that are typically hard to handle for tree-based algorithms (because many decision boundaries are required).\n",
    "\n",
    "_Note: this method also works with regression problems._\n",
    "\n",
    "When increasing the maximum depth of trees leads, we can expect our models to overfit the training set. If the performance also increases for the validation set, it means that our model needs a huge number of splits to extract information from some variables. In this case, mean encoding is likely to provide significant benefits to our model.\n",
    "\n",
    "Mean encoding can be performed in several ways:\n",
    "+ likelihood (target mean): P / (N + P)\n",
    "+ weight of evidence: ln(P/N) * 100\n",
    "+ count: P\n",
    "+ diff: P - N\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# mean encoding must be performed on the training set only \n",
    "means = df.groupby(feature_col).target.mean()\n",
    "\n",
    "# it is the napplied to both training and validation sets\n",
    "df_train[feature_col + '_mean_target'] = df_train[feature_col].map(means)\n",
    "df_val[feature_col + '_mean_target'] = df_val[feature_col].map(means)\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Naive mean encoding might lead to overfitting when, for instance, all observations of a given category have the same target value in the training set but not in the validation set. Regularization can help.\n",
    "\n",
    "+ CV loop: use the entire dataset to build the mean encoding, but only for the validation set of each CV fold. This technique leaks information from the validation set to the training set, hasn't any major impact if the data is large enough compared to the number of folds. This is why it is recommended to only use a small CV of 4-5 folds.\n",
    "+ smoothing: use mean encoding only for categories with a lot of rows (see formula below). Must be combined with another method like CV loop to prevent overfitting.\n",
    "+ expanding mean: sort the data and only consider the [0..n-1] rows to calculate the mean encoding of row n. The feature quality is not always excellent, but we can compensate by averaging the predictions of models fitted on encodings calculated from different data permutations.\n",
    "\n",
    "$$\\mathrm{smoothing} = \\frac{\\mathrm{mean}(\\mathrm{target}) * \\mathrm{nrows} + \\mathrm{global\\_mean} * \\alpha}{\\mathrm{nrows} + \\alpha}$$\n",
    "\n",
    "_Note: the expanding mean is built-in in the library CatBoost that works great for categorical datasets._\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## III.4. Feature Encoding\n",
    "\n",
    "The mean is the only meaningful statistic we can extract from a classification variable. \n",
    "\n",
    "For multiclassification tasks, we will create n features: one mean encoding for each class. This has the advantage of giving tree-based algorithms information about other classes where typically they don't: they usually solve multiclassification as n problems of one versus all so every class has a different model.\n",
    "\n",
    "For regression tasks, we have much more options: percentiles, standard deviation, etc. We can also use distribution bins: create x features that count the number of times the target variable is in the x-th distribution bin.\n",
    "\n",
    "For many-to-many combinations (like classification problems from apps installed on an user's phone), we can mean-encode based on each user-app combination (long-form representation), then merge the results into a vector for each user. We can apply various statistics to these vectors, like mean or standard deviation.\n",
    "\n",
    "For time series, calculating some statistic of previous values can also help significantly.\n",
    "\n",
    "It can also be useful to investigate numerical features that have many splits: try to mean-encode binned values where bins come from the splits. Another option is to mean-encode feature interactions between features that are frequently in neighboring nodes.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we assume that the df is the available dataset \n",
    "# and that we have created an empty df_new df\n",
    "# that will include all our new features\n",
    "\n",
    "# CV loop\n",
    "skf = StratifiedKFold(y, 5, shuffle=True, random_state=42)\n",
    "\n",
    "for tr_idx, val_idx in skf:\n",
    "    # split fold in train/val\n",
    "    df_tr, df_val = df.iloc[tr_idx], df.iloc[val_idx]\n",
    "    # loop over all feature columns to encode\n",
    "    for feature_col in feature_cols: \n",
    "        means = df_tr.groupby(feature_col)['target'].mean()\n",
    "        df_val[feature_col + '_mean_target'] = df_val[feature_col].map(means)\n",
    "    # save to df_new\n",
    "    df_new.iloc[val_idx] = df_val\n",
    "\n",
    "# fill missing values with the global target mean\n",
    "prior = df['target'].mean()\n",
    "df_new.fillna(prior, inplace=True)\n",
    "\n",
    "# expanding mean\n",
    "cumsum = df.groupby(feature_col)['target'].cumsum() - df['target'] # cumsum of [0..n-1]\n",
    "cumcnt = df.groupby(feature_col)['target'].cumcount()              # cumcnt of [0..n-1]\n",
    "df_new[feature_col + 'mean_target'] = cumsum / cumcnt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Features Generation\r\n",
    "## Common Methods\n",
    "\n",
    "\r\n",
    "A good explanation of the benefits of features engineering, in addition to examples, can be found [here](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/) and [here](https://www.quora.com/What-are-some-best-practices-in-Feature-Engineering).\r\n",
    "\r\n",
    "_Note: we can create a new feature that flags incorrect values if they appear to follow some pattern (as opposed to typos for instance\n",
    "**Numerical**\n",
    "+ combining several features into one via division or multiplication (area from width and lenght, price per square meter).\n",
    "+ extracting the fractional part of prices (i.e. cents) to capture the psychology of how people perceive these prices.\n",
    "+ flagging non-human behaviors (bots) by looking at patterns (non-rounded prices at auctions, constant intervals in messages sent on social media).\n",
    "\n",
    "**Categorical & Ordinal**\n",
    "+ combining several categorical features into one (features interaction).\n",
    "\n",
    "**Datetimes**\n",
    "+ periodicity (day of week, day of year, day, month, season, year). Helps capture repetitive patterns in the data.\n",
    "+ time passed since major event (days since last holidays, is_holiday boolean).\n",
    "+ difference between dates.\n",
    "\n",
    "**Coordinates**\n",
    "+ distance to major landmarks (shops, subway station).\n",
    "+ distance to most remarkable data point of the dataset split into clusters (most expensive flat).\n",
    "+ distance to the center of the dataset split into clusters.\n",
    "+ aggregated statistics for the area around each point (number of flats in a given radius, mean price per square meter).\n",
    "+ rotate coordinates so decision trees have decision boundaries closer to horizontal/vertical.\n",
    "\n",
    "**Text Feature Extraction**\n",
    "\n",
    "Text (see [NLTK](https://www.nltk.org/)):\n",
    "+ Lemming (use root word for all its variants and related words), stemming (truncate the end of words), stopwords removal, lowercase.\n",
    "+ Bag of Words: count occurrences of each word of a given dictionary for each sentence (`sklearn.feature_extraction.text.CountVectorizer`).\n",
    "+ Bag of Words TFiDF (`sklearn.feature_extraction.text.TfidfVectorizer` - see code below).\n",
    "+ N-grams (`sklearn.feature_extraction.text.CountVectorizer(Ngram_range, analyzer)`).\n",
    "+ Word2Vec Embedding: convert words into a vector. Operations are possible to extract additional meaning (king - man + woman = queen).\n",
    ")._\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words - terms frequency (texts of different sizes become more comparable)\n",
    "tf = 1 / x.sum(axis=1)[:, None] # number of words per row\n",
    "x = x * tf\n",
    "\n",
    "# bag of words - inverse document frequency (boost more important features/words - that is, less frequent words)\n",
    "idf = np.log(len(x) / (x > 0).sum(axis=0)) # inverse of the fraction of rows with the word\n",
    "x = x * idf\n"
   ]
  },
  {
   "source": [
    "## Statistics and Distance-Based Features\n",
    "\n",
    "We can add new features based on relations between data points. For instance, we can calculate the number of web pages a specific user has visited during a specific session, or the minimum and maximum price for articles displayed on a specific page. In other words, we group some features by value then calculate summary statistics of other features for each of these values.\n",
    "\n",
    "Another method, more flexible but harder to implement, is to calculate summary statistics of neighboring values. For instance, we can estimate the average price per square meter of a specific neighborood by looking at the coordinates of each house in the dataset.\n",
    "\n",
    "\n",
    "## Feature Interaction\n",
    "\n",
    "This method combines two features into one to make interactions more explicit. This is especially useful for tree-based methods that would struggle to capture these interactions otherwise. \n",
    "\n",
    "+ For categorical variables, the method concatenates the values into one. \n",
    "+ For numerical variables, any operation that takes two arguments would work: sum, multiplication, division, etc.\n",
    "\n",
    "Not all interactions are relevant. We can fit a tree-based method to a dataset with all possible interactions and use the ranking of features importance to keep the most useful ones only.\n",
    "\n",
    "Lastly, tree-based models can use the index of each tree leaf to identify high-order interactions (i.e. branches with few leaves vs many leaves). The implementation is simple:\n",
    "+ sklearn: `tree_model.apply()`.\n",
    "+ xgboost: `booster.predict(pred_leaf=true)`.\n",
    "\n",
    "An implementation example can be found [here](https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "## Matrix Factorization\n",
    "\n",
    "Matrix Factorization is a general approach to dimensionality reduction and feature extraction.\n",
    "\n",
    "The most common features of matrix factorization are [Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) (SVD) and [Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA). Other methods include TruncatedSVD for sparse matrices (like text classification) and Non-Negative Matrix Factorization (NMF) for counts-like data (like Bag-of-Words matrices).\n",
    "\n",
    "More details about the different methods of matrix factorization can be found [here](https://scikit-learn.org/stable/modules/decomposition.html).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca parameters\n",
    "pca = PCA(n_components=5)\n",
    "\n",
    "# pca done the wrong way\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.fit_transform(X_test)\n",
    "\n",
    "# pca done the right way\n",
    "X_all = np.concatenate([X_train, X_test])\n",
    "pca.fit(X_all)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n"
   ]
  },
  {
   "source": [
    "## t-SNE\n",
    "\n",
    "Matrix Factorization is a linear method of dimentionality reduction; t-SNE is a non-linear method that can be very powerful for vizualization purposes. A few points to keep in mind: \n",
    "+ results depend heavily on the perplexity value.\n",
    "+ due to its stochastic nature, t-SNE provides different projections every time it is run, which is why train and test sets hould be projected together.\n",
    "\n",
    "Additional resources:\n",
    "+ More details about the different methods of manifold learning methods can be found [here](https://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html).\n",
    "+ An implementation example, with its code, can be found [here](https://scikit-learn.org/stable/auto_examples/manifold/plot_t_sne_perplexity.html#sphx-glr-auto-examples-manifold-plot-t-sne-perplexity-py). \n",
    "+ This [page](https://distill.pub/2016/misread-tsne/) illustrates the effects of various parameters, as well as interactive plots to explore those effects.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# VI. Competition-specific Steps\n",
    "\n",
    "A few extra steps can be taken during competitions:\n",
    "\n",
    "+ We can remove all features that are either constant in the training set or a duplicate of another column (`df.T.drop_duplicates()`). For categorical features, we'll need to label encode them all by order of appearance.\n",
    "+ If categorical values only exist in the test set, we need to test if these new values bring any useful information. One possibility is to compare the model performance on a validation set, for values in the training set vs previously unseen values. We might want to use a different model for new values if performance is low for them. \n",
    "+ Duplicated rows can either be mistakes or the result of having a key feature ommitted from the dataset. Having identical rows in both train and test sets can help us understand how the data was generated.\n",
    "+ When plotting rolling mean of values by index, we can check if there are some patterns that will indicate that the data was not properly shuffled.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 32-bit",
   "name": "python38032bit64a64ed7a47843b8be3706a54e9a0958"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}